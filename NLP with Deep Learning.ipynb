{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scipy: 1.0.0\n",
      "numpy: 1.11.3\n",
      "matplotlib: 2.0.0\n",
      "pandas: 0.19.2\n",
      "statsmodels: 0.6.1\n",
      "sklearn: 0.19.1\n",
      "tensorflow: 1.4.0\n",
      "keras: 2.1.2\n"
     ]
    }
   ],
   "source": [
    "import scipy\n",
    "import numpy\n",
    "import matplotlib\n",
    "import pandas\n",
    "import statsmodels\n",
    "import sklearn\n",
    "#import theano\n",
    "import tensorflow\n",
    "import keras\n",
    "\n",
    "print('scipy: %s' % scipy.__version__)\n",
    "print('numpy: %s' % numpy.__version__)\n",
    "print('matplotlib: %s' % matplotlib.__version__)\n",
    "print('pandas: %s' % pandas.__version__)\n",
    "print('statsmodels: %s' % statsmodels.__version__)\n",
    "print('sklearn: %s' % sklearn.__version__)\n",
    "#print('theano: %s' % theano.__version__)\n",
    "print('tensorflow: %s' % tensorflow.__version__)\n",
    "print('keras: %s' % keras.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### split into words by white space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['One', 'morning,', 'when', 'Gregor', 'Samsa', 'woke', 'from', 'troubled', 'dreams,', 'he', 'found', 'himself', 'transformed', 'in', 'his', 'bed', 'into', 'a', 'horrible', 'vermin.', 'He', 'lay', 'on', 'his', 'armour-like', 'back,', 'and', 'if', 'he', 'lifted', 'his', 'head', 'a', 'little', 'he', 'could', 'see', 'his', 'brown', 'belly,', 'slightly', 'domed', 'and', 'divided', 'by', 'arches', 'into', 'stiff', 'sections.', 'The', 'bedding', 'was', 'hardly', 'able', 'to', 'cover', 'it', 'and', 'seemed', 'ready', 'to', 'slide', 'off', 'any', 'moment.', 'His', 'many', 'legs,', 'pitifully', 'thin', 'compared', 'with', 'the', 'size', 'of', 'the', 'rest', 'of', 'him,', 'waved', 'about', 'helplessly', 'as', 'he', 'looked.', '\"What\\'s', 'happened', 'to', 'me?\"', 'he', 'thought.', 'It', \"wasn't\", 'a', 'dream.', 'His', 'room,', 'a', 'proper', 'human']\n"
     ]
    }
   ],
   "source": [
    "# load text\n",
    "filename = 'metamorphosis_clean.txt'\n",
    "file = open(filename, 'rt')\n",
    "text = file.read()\n",
    "file.close()\n",
    "# split into words by white space\n",
    "words = text.split()\n",
    "print(words[:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### split based on words only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['One', 'morning', 'when', 'Gregor', 'Samsa', 'woke', 'from', 'troubled', 'dreams', 'he', 'found', 'himself', 'transformed', 'in', 'his', 'bed', 'into', 'a', 'horrible', 'vermin', 'He', 'lay', 'on', 'his', 'armour', 'like', 'back', 'and', 'if', 'he', 'lifted', 'his', 'head', 'a', 'little', 'he', 'could', 'see', 'his', 'brown', 'belly', 'slightly', 'domed', 'and', 'divided', 'by', 'arches', 'into', 'stiff', 'sections', 'The', 'bedding', 'was', 'hardly', 'able', 'to', 'cover', 'it', 'and', 'seemed', 'ready', 'to', 'slide', 'off', 'any', 'moment', 'His', 'many', 'legs', 'pitifully', 'thin', 'compared', 'with', 'the', 'size', 'of', 'the', 'rest', 'of', 'him', 'waved', 'about', 'helplessly', 'as', 'he', 'looked', 'What', 's', 'happened', 'to', 'me', 'he', 'thought', 'It', 'wasn', 't', 'a', 'dream', 'His', 'room']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "# load text\n",
    "filename = 'metamorphosis_clean.txt'\n",
    "file = open(filename, 'rt')\n",
    "text = file.read()\n",
    "file.close()\n",
    "# split based on words only\n",
    "words = re.split(r'\\W+', text)\n",
    "print(words[:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### manually remove punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['One', 'morning', 'when', 'Gregor', 'Samsa', 'woke', 'from', 'troubled', 'dreams', 'he', 'found', 'himself', 'transformed', 'in', 'his', 'bed', 'into', 'a', 'horrible', 'vermin', 'He', 'lay', 'on', 'his', 'armourlike', 'back', 'and', 'if', 'he', 'lifted', 'his', 'head', 'a', 'little', 'he', 'could', 'see', 'his', 'brown', 'belly', 'slightly', 'domed', 'and', 'divided', 'by', 'arches', 'into', 'stiff', 'sections', 'The', 'bedding', 'was', 'hardly', 'able', 'to', 'cover', 'it', 'and', 'seemed', 'ready', 'to', 'slide', 'off', 'any', 'moment', 'His', 'many', 'legs', 'pitifully', 'thin', 'compared', 'with', 'the', 'size', 'of', 'the', 'rest', 'of', 'him', 'waved', 'about', 'helplessly', 'as', 'he', 'looked', 'Whats', 'happened', 'to', 'me', 'he', 'thought', 'It', 'wasnt', 'a', 'dream', 'His', 'room', 'a', 'proper', 'human']\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "import re\n",
    "# load text\n",
    "filename = 'metamorphosis_clean.txt'\n",
    "file = open(filename, 'rt')\n",
    "text = file.read()\n",
    "file.close()\n",
    "# split into words by white space\n",
    "words = text.split()\n",
    "# prepare regex for char filtering\n",
    "re_punc = re.compile('[%s]' % re.escape(string.punctuation))\n",
    "# remove punctuation from each word\n",
    "stripped = [re_punc.sub('', w) for w in words]\n",
    "print(stripped[:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### converting to lower case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['one', 'morning,', 'when', 'gregor', 'samsa', 'woke', 'from', 'troubled', 'dreams,', 'he', 'found', 'himself', 'transformed', 'in', 'his', 'bed', 'into', 'a', 'horrible', 'vermin.', 'he', 'lay', 'on', 'his', 'armour-like', 'back,', 'and', 'if', 'he', 'lifted', 'his', 'head', 'a', 'little', 'he', 'could', 'see', 'his', 'brown', 'belly,', 'slightly', 'domed', 'and', 'divided', 'by', 'arches', 'into', 'stiff', 'sections.', 'the', 'bedding', 'was', 'hardly', 'able', 'to', 'cover', 'it', 'and', 'seemed', 'ready', 'to', 'slide', 'off', 'any', 'moment.', 'his', 'many', 'legs,', 'pitifully', 'thin', 'compared', 'with', 'the', 'size', 'of', 'the', 'rest', 'of', 'him,', 'waved', 'about', 'helplessly', 'as', 'he', 'looked.', '\"what\\'s', 'happened', 'to', 'me?\"', 'he', 'thought.', 'it', \"wasn't\", 'a', 'dream.', 'his', 'room,', 'a', 'proper', 'human']\n"
     ]
    }
   ],
   "source": [
    "filename = 'metamorphosis_clean.txt'\n",
    "file = open(filename, 'rt')\n",
    "text = file.read()\n",
    "file.close()\n",
    "# split into words by white space\n",
    "words = text.split()\n",
    "# convert to lower case\n",
    "words = [word.lower() for word in words]\n",
    "print(words[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "showing info https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/index.xml\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### split sentences using nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "One morning, when Gregor Samsa woke from troubled dreams, he found\n",
      "himself transformed in his bed into a horrible vermin.\n"
     ]
    }
   ],
   "source": [
    "from nltk import sent_tokenize\n",
    "# load data\n",
    "filename = 'metamorphosis_clean.txt'\n",
    "file = open(filename, 'rt')\n",
    "text = file.read()\n",
    "file.close()\n",
    "# split into sentences\n",
    "sentences = sent_tokenize(text)\n",
    "print(sentences[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### split words using nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['One', 'morning', ',', 'when', 'Gregor', 'Samsa', 'woke', 'from', 'troubled', 'dreams', ',', 'he', 'found', 'himself', 'transformed', 'in', 'his', 'bed', 'into', 'a', 'horrible', 'vermin', '.', 'He', 'lay', 'on', 'his', 'armour-like', 'back', ',', 'and', 'if', 'he', 'lifted', 'his', 'head', 'a', 'little', 'he', 'could', 'see', 'his', 'brown', 'belly', ',', 'slightly', 'domed', 'and', 'divided', 'by', 'arches', 'into', 'stiff', 'sections', '.', 'The', 'bedding', 'was', 'hardly', 'able', 'to', 'cover', 'it', 'and', 'seemed', 'ready', 'to', 'slide', 'off', 'any', 'moment', '.', 'His', 'many', 'legs', ',', 'pitifully', 'thin', 'compared', 'with', 'the', 'size', 'of', 'the', 'rest', 'of', 'him', ',', 'waved', 'about', 'helplessly', 'as', 'he', 'looked', '.', '``', 'What', \"'s\", 'happened', 'to']\n",
      "4\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "# load data\n",
    "filename = 'metamorphosis_clean.txt'\n",
    "file = open(filename, 'rt')\n",
    "text = file.read()\n",
    "file.close()\n",
    "# split into words\n",
    "tokens = word_tokenize(text)\n",
    "print(tokens[:100])\n",
    "print(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### remove punctuation using nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['One', 'morning', 'when', 'Gregor', 'Samsa', 'woke', 'from', 'troubled', 'dreams', 'he', 'found', 'himself', 'transformed', 'in', 'his', 'bed', 'into', 'a', 'horrible', 'vermin', 'He', 'lay', 'on', 'his', 'back', 'and', 'if', 'he', 'lifted', 'his', 'head', 'a', 'little', 'he', 'could', 'see', 'his', 'brown', 'belly', 'slightly', 'domed', 'and', 'divided', 'by', 'arches', 'into', 'stiff', 'sections', 'The', 'bedding', 'was', 'hardly', 'able', 'to', 'cover', 'it', 'and', 'seemed', 'ready', 'to', 'slide', 'off', 'any', 'moment', 'His', 'many', 'legs', 'pitifully', 'thin', 'compared', 'with', 'the', 'size', 'of', 'the', 'rest', 'of', 'him', 'waved', 'about', 'helplessly', 'as', 'he', 'looked', 'What', 'happened', 'to', 'me', 'he', 'thought', 'It', 'was', 'a', 'dream', 'His', 'room', 'a', 'proper', 'human', 'room']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "# load data\n",
    "filename = 'metamorphosis_clean.txt'\n",
    "file = open(filename, 'rt')\n",
    "text = file.read()\n",
    "file.close()\n",
    "# split into words\n",
    "tokens = word_tokenize(text)\n",
    "# remove all tokens that are not alphabetic\n",
    "words = [word for word in tokens if word.isalpha()]\n",
    "print(words[:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stop_words = stopwords.words('english')\n",
    "print(stop_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### filtering stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['one', 'morning', 'gregor', 'samsa', 'woke', 'troubled', 'dreams', 'found', 'transformed', 'bed', 'horrible', 'vermin', 'lay', 'armourlike', 'back', 'lifted', 'head', 'little', 'could', 'see', 'brown', 'belly', 'slightly', 'domed', 'divided', 'arches', 'stiff', 'sections', 'bedding', 'hardly', 'able', 'cover', 'seemed', 'ready', 'slide', 'moment', 'many', 'legs', 'pitifully', 'thin', 'compared', 'size', 'rest', 'waved', 'helplessly', 'looked', 'happened', 'thought', 'nt', 'dream', 'room', 'proper', 'human', 'room', 'although', 'little', 'small', 'lay', 'peacefully', 'four', 'familiar', 'walls', 'collection', 'textile', 'samples', 'lay', 'spread', 'table', 'samsa', 'travelling', 'salesman', 'hung', 'picture', 'recently', 'cut', 'illustrated', 'magazine', 'housed', 'nice', 'gilded', 'frame', 'showed', 'lady', 'fitted', 'fur', 'hat', 'fur', 'boa', 'sat', 'upright', 'raising', 'heavy', 'fur', 'muff', 'covered', 'whole', 'lower', 'arm', 'towards', 'viewer']\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "import re\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "# load data\n",
    "filename = 'metamorphosis_clean.txt'\n",
    "file = open(filename, 'rt')\n",
    "text = file.read()\n",
    "file.close()\n",
    "# split into words\n",
    "tokens = word_tokenize(text)\n",
    "# convert to lower case\n",
    "tokens = [w.lower() for w in tokens]\n",
    "# prepare regex for char filtering\n",
    "re_punc = re.compile('[%s]' % re.escape(string.punctuation))\n",
    "# remove punctuation from each word\n",
    "stripped = [re_punc.sub('', w) for w in tokens]\n",
    "# remove remaining tokens that are not alphabetic\n",
    "words = [word for word in stripped if word.isalpha()]\n",
    "# filter out stop words\n",
    "stop_words = set(stopwords.words('english'))\n",
    "words = [w for w in words if not w in stop_words]\n",
    "print(words[:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['one', 'morn', ',', 'when', 'gregor', 'samsa', 'woke', 'from', 'troubl', 'dream', ',', 'he', 'found', 'himself', 'transform', 'in', 'hi', 'bed', 'into', 'a', 'horribl', 'vermin', '.', 'He', 'lay', 'on', 'hi', 'armour-lik', 'back', ',', 'and', 'if', 'he', 'lift', 'hi', 'head', 'a', 'littl', 'he', 'could', 'see', 'hi', 'brown', 'belli', ',', 'slightli', 'dome', 'and', 'divid', 'by', 'arch', 'into', 'stiff', 'section', '.', 'the', 'bed', 'wa', 'hardli', 'abl', 'to', 'cover', 'it', 'and', 'seem', 'readi', 'to', 'slide', 'off', 'ani', 'moment', '.', 'hi', 'mani', 'leg', ',', 'piti', 'thin', 'compar', 'with', 'the', 'size', 'of', 'the', 'rest', 'of', 'him', ',', 'wave', 'about', 'helplessli', 'as', 'he', 'look', '.', '``', 'what', \"'s\", 'happen', 'to']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "# load data\n",
    "filename = 'metamorphosis_clean.txt'\n",
    "file = open(filename, 'rt')\n",
    "text = file.read()\n",
    "file.close()\n",
    "# split into words\n",
    "tokens = word_tokenize(text)\n",
    "# stemming of words\n",
    "porter = PorterStemmer()\n",
    "stemmed = [porter.stem(word) for word in tokens]\n",
    "print(stemmed[:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### countvectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'the': 7, 'quick': 6, 'brown': 0, 'fox': 2, 'jumped': 3, 'over': 5, 'lazy': 4, 'dog': 1}\n",
      "(1, 8)\n",
      "<class 'scipy.sparse.csr.csr_matrix'>\n",
      "[[1 1 1 1 1 1 1 2]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "# list of text documents\n",
    "text = [\"The quick brown fox jumped over the lazy dog.\"]\n",
    "# create the transform\n",
    "vectorizer = CountVectorizer()\n",
    "# tokenize and build vocab\n",
    "vectorizer.fit(text)\n",
    "# summarize\n",
    "print(vectorizer.vocabulary_)\n",
    "# encode document\n",
    "vector = vectorizer.transform(text)\n",
    "# summarize encoded vector\n",
    "print(vector.shape)\n",
    "print(type(vector))\n",
    "print(vector.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'the': 7, 'quick': 6, 'brown': 0, 'fox': 2, 'jumped': 3, 'over': 5, 'lazy': 4, 'dog': 1}\n",
      "[ 1.69314718  1.28768207  1.28768207  1.69314718  1.69314718  1.69314718\n",
      "  1.69314718  1.        ]\n",
      "(1, 8)\n",
      "[[ 0.36388646  0.27674503  0.27674503  0.36388646  0.36388646  0.36388646\n",
      "   0.36388646  0.42983441]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "# list of text documents\n",
    "text = [\"The quick brown fox jumped over the lazy dog.\",\n",
    "\t\t\"The dog.\",\n",
    "\t\t\"The fox\"]\n",
    "# create the transform\n",
    "vectorizer = TfidfVectorizer()\n",
    "# tokenize and build vocab\n",
    "vectorizer.fit(text)\n",
    "# summarize\n",
    "print(vectorizer.vocabulary_)\n",
    "print(vectorizer.idf_)\n",
    "# encode document\n",
    "vector = vectorizer.transform([text[0]])\n",
    "# summarize encoded vector\n",
    "print(vector.shape)\n",
    "print(vector.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### hashvectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 20)\n",
      "[[ 0.          0.          0.          0.          0.          0.33333333\n",
      "   0.         -0.33333333  0.33333333  0.          0.          0.33333333\n",
      "   0.          0.          0.         -0.33333333  0.          0.\n",
      "  -0.66666667  0.        ]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "# list of text documents\n",
    "text = [\"The quick brown fox jumped over the lazy dog.\"]\n",
    "# create the transform\n",
    "vectorizer = HashingVectorizer(n_features=20)\n",
    "# encode document\n",
    "vector = vectorizer.transform(text)\n",
    "# summarize encoded vector\n",
    "print(vector.shape)\n",
    "print(vector.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### split text into word sequence using keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['the', 'quick', 'brown', 'fox', 'jumped', 'over', 'the', 'lazy', 'dog']\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.text import text_to_word_sequence\n",
    "# define the document\n",
    "text = 'The quick brown fox jumped over the lazy dog.'\n",
    "# tokenize the document\n",
    "result = text_to_word_sequence(text)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### calculating the vocab size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.text import text_to_word_sequence\n",
    "# define the document\n",
    "text = 'The quick brown fox jumped over the lazy dog.'\n",
    "# estimate the size of the vocabulary\n",
    "words = set(text_to_word_sequence(text))\n",
    "vocab_size = len(words)\n",
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### one hot encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n",
      "[3, 3, 6, 6, 2, 4, 3, 8, 7]\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.text import one_hot\n",
    "from keras.preprocessing.text import text_to_word_sequence\n",
    "# define the document\n",
    "text = 'The quick brown fox jumped over the lazy dog.'\n",
    "# estimate the size of the vocabulary\n",
    "words = set(text_to_word_sequence(text))\n",
    "vocab_size = len(words)\n",
    "print(vocab_size)\n",
    "# integer encode the document\n",
    "result = one_hot(text, round(vocab_size*1.3))\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### hash encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n",
      "[6, 4, 1, 2, 7, 5, 6, 2, 6]\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.text import hashing_trick\n",
    "from keras.preprocessing.text import text_to_word_sequence\n",
    "# define the document\n",
    "text = 'The quick brown fox jumped over the lazy dog.'\n",
    "# estimate the size of the vocabulary\n",
    "words = set(text_to_word_sequence(text))\n",
    "vocab_size = len(words)\n",
    "print(vocab_size)\n",
    "# integer encode the document\n",
    "result = hashing_trick(text, round(vocab_size*1.3), hash_function='md5')\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OrderedDict([('well', 1), ('done', 1), ('good', 1), ('work', 2), ('great', 1), ('effort', 1), ('nice', 1), ('excellent', 1)])\n",
      "5\n",
      "{'work': 1, 'well': 2, 'done': 3, 'good': 4, 'great': 5, 'effort': 6, 'nice': 7, 'excellent': 8}\n",
      "{'done': 1, 'well': 1, 'good': 1, 'work': 2, 'great': 1, 'effort': 1, 'nice': 1, 'excellent': 1}\n",
      "[[ 0.  0.  1.  1.  0.  0.  0.  0.  0.]\n",
      " [ 0.  1.  0.  0.  1.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  1.  1.  0.  0.]\n",
      " [ 0.  1.  0.  0.  0.  0.  0.  1.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  1.]]\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "# define 5 documents\n",
    "docs = ['Well done!','Good work','Great effort','nice work','Excellent!']\n",
    "# create the tokenizer\n",
    "t = Tokenizer()\n",
    "# fit the tokenizer on the documents\n",
    "t.fit_on_texts(docs)\n",
    "# summarize what was learned\n",
    "print(t.word_counts)\n",
    "print(t.document_count)\n",
    "print(t.word_index)\n",
    "print(t.word_docs)\n",
    "# integer encode documents\n",
    "encoded_docs = t.texts_to_matrix(docs, mode='count')\n",
    "print(encoded_docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare Movie Review Data for Sentiment Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### load file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# load one file\n",
    "filename = 'cv000_29416.txt'\n",
    "# open the file as read only\n",
    "file = open(filename, 'r')\n",
    "# read all text\n",
    "text = file.read()\n",
    "# close the file\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### load all files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from os import listdir\n",
    "\n",
    "# load doc into memory\n",
    "def load_doc(filename):\n",
    "\t# open the file as read only\n",
    "\tfile = open(filename, 'r')\n",
    "\t# read all text\n",
    "\ttext = file.read()\n",
    "\t# close the file\n",
    "\tfile.close()\n",
    "\treturn text\n",
    "\n",
    "# specify directory to load\n",
    "directory = 'txt_sentoken/neg'\n",
    "# walk through all files in the folder\n",
    "for filename in listdir(directory):\n",
    "\t# skip files that do not have the right extension\n",
    "\tif not filename.endswith(\".txt\"):\n",
    "\t\tnext\n",
    "\t# create the full path of the file to open\n",
    "\tpath = directory + '/' + filename\n",
    "\t# load document\n",
    "\tdoc = load_doc(path)\n",
    "\tprint('Loaded %s' % filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### load all files with function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded cv000_29416.txt\n",
      "Loaded cv001_19502.txt\n",
      "Loaded cv002_17424.txt\n",
      "Loaded cv003_12683.txt\n",
      "Loaded cv004_12641.txt\n",
      "Loaded cv005_29357.txt\n",
      "Loaded cv006_17022.txt\n",
      "Loaded cv007_4992.txt\n",
      "Loaded cv008_29326.txt\n",
      "Loaded cv009_29417.txt\n",
      "Loaded cv010_29063.txt\n",
      "Loaded cv011_13044.txt\n",
      "Loaded cv012_29411.txt\n",
      "Loaded cv013_10494.txt\n",
      "Loaded cv014_15600.txt\n",
      "Loaded cv015_29356.txt\n",
      "Loaded cv016_4348.txt\n",
      "Loaded cv017_23487.txt\n",
      "Loaded cv018_21672.txt\n",
      "Loaded cv019_16117.txt\n",
      "Loaded cv020_9234.txt\n",
      "Loaded cv021_17313.txt\n",
      "Loaded cv022_14227.txt\n",
      "Loaded cv023_13847.txt\n",
      "Loaded cv024_7033.txt\n",
      "Loaded cv025_29825.txt\n",
      "Loaded cv026_29229.txt\n",
      "Loaded cv027_26270.txt\n",
      "Loaded cv028_26964.txt\n",
      "Loaded cv029_19943.txt\n",
      "Loaded cv030_22893.txt\n",
      "Loaded cv031_19540.txt\n",
      "Loaded cv032_23718.txt\n",
      "Loaded cv033_25680.txt\n",
      "Loaded cv034_29446.txt\n",
      "Loaded cv035_3343.txt\n",
      "Loaded cv036_18385.txt\n",
      "Loaded cv037_19798.txt\n",
      "Loaded cv038_9781.txt\n",
      "Loaded cv039_5963.txt\n",
      "Loaded cv040_8829.txt\n",
      "Loaded cv041_22364.txt\n",
      "Loaded cv042_11927.txt\n",
      "Loaded cv043_16808.txt\n",
      "Loaded cv044_18429.txt\n",
      "Loaded cv045_25077.txt\n",
      "Loaded cv046_10613.txt\n",
      "Loaded cv047_18725.txt\n",
      "Loaded cv048_18380.txt\n",
      "Loaded cv049_21917.txt\n",
      "Loaded cv050_12128.txt\n",
      "Loaded cv051_10751.txt\n",
      "Loaded cv052_29318.txt\n",
      "Loaded cv053_23117.txt\n",
      "Loaded cv054_4101.txt\n",
      "Loaded cv055_8926.txt\n",
      "Loaded cv056_14663.txt\n",
      "Loaded cv057_7962.txt\n",
      "Loaded cv058_8469.txt\n",
      "Loaded cv059_28723.txt\n",
      "Loaded cv060_11754.txt\n",
      "Loaded cv061_9321.txt\n",
      "Loaded cv062_24556.txt\n",
      "Loaded cv063_28852.txt\n",
      "Loaded cv064_25842.txt\n",
      "Loaded cv065_16909.txt\n",
      "Loaded cv066_11668.txt\n",
      "Loaded cv067_21192.txt\n",
      "Loaded cv068_14810.txt\n",
      "Loaded cv069_11613.txt\n",
      "Loaded cv070_13249.txt\n",
      "Loaded cv071_12969.txt\n",
      "Loaded cv072_5928.txt\n",
      "Loaded cv073_23039.txt\n",
      "Loaded cv074_7188.txt\n",
      "Loaded cv075_6250.txt\n",
      "Loaded cv076_26009.txt\n",
      "Loaded cv077_23172.txt\n",
      "Loaded cv078_16506.txt\n",
      "Loaded cv079_12766.txt\n",
      "Loaded cv080_14899.txt\n",
      "Loaded cv081_18241.txt\n",
      "Loaded cv082_11979.txt\n",
      "Loaded cv083_25491.txt\n",
      "Loaded cv084_15183.txt\n",
      "Loaded cv085_15286.txt\n",
      "Loaded cv086_19488.txt\n",
      "Loaded cv087_2145.txt\n",
      "Loaded cv088_25274.txt\n",
      "Loaded cv089_12222.txt\n",
      "Loaded cv090_0049.txt\n",
      "Loaded cv091_7899.txt\n",
      "Loaded cv092_27987.txt\n",
      "Loaded cv093_15606.txt\n",
      "Loaded cv094_27868.txt\n",
      "Loaded cv095_28730.txt\n",
      "Loaded cv096_12262.txt\n",
      "Loaded cv097_26081.txt\n",
      "Loaded cv098_17021.txt\n",
      "Loaded cv099_11189.txt\n",
      "Loaded cv100_12406.txt\n",
      "Loaded cv101_10537.txt\n",
      "Loaded cv102_8306.txt\n",
      "Loaded cv103_11943.txt\n",
      "Loaded cv104_19176.txt\n",
      "Loaded cv105_19135.txt\n",
      "Loaded cv106_18379.txt\n",
      "Loaded cv107_25639.txt\n",
      "Loaded cv108_17064.txt\n",
      "Loaded cv109_22599.txt\n",
      "Loaded cv110_27832.txt\n",
      "Loaded cv111_12253.txt\n",
      "Loaded cv112_12178.txt\n",
      "Loaded cv113_24354.txt\n",
      "Loaded cv114_19501.txt\n",
      "Loaded cv115_26443.txt\n",
      "Loaded cv116_28734.txt\n",
      "Loaded cv117_25625.txt\n",
      "Loaded cv118_28837.txt\n",
      "Loaded cv119_9909.txt\n",
      "Loaded cv120_3793.txt\n",
      "Loaded cv121_18621.txt\n",
      "Loaded cv122_7891.txt\n",
      "Loaded cv123_12165.txt\n",
      "Loaded cv124_3903.txt\n",
      "Loaded cv125_9636.txt\n",
      "Loaded cv126_28821.txt\n",
      "Loaded cv127_16451.txt\n",
      "Loaded cv128_29444.txt\n",
      "Loaded cv129_18373.txt\n",
      "Loaded cv130_18521.txt\n",
      "Loaded cv131_11568.txt\n",
      "Loaded cv132_5423.txt\n",
      "Loaded cv133_18065.txt\n",
      "Loaded cv134_23300.txt\n",
      "Loaded cv135_12506.txt\n",
      "Loaded cv136_12384.txt\n",
      "Loaded cv137_17020.txt\n",
      "Loaded cv138_13903.txt\n",
      "Loaded cv139_14236.txt\n",
      "Loaded cv140_7963.txt\n",
      "Loaded cv141_17179.txt\n",
      "Loaded cv142_23657.txt\n",
      "Loaded cv143_21158.txt\n",
      "Loaded cv144_5010.txt\n",
      "Loaded cv145_12239.txt\n",
      "Loaded cv146_19587.txt\n",
      "Loaded cv147_22625.txt\n",
      "Loaded cv148_18084.txt\n",
      "Loaded cv149_17084.txt\n",
      "Loaded cv150_14279.txt\n",
      "Loaded cv151_17231.txt\n",
      "Loaded cv152_9052.txt\n",
      "Loaded cv153_11607.txt\n",
      "Loaded cv154_9562.txt\n",
      "Loaded cv155_7845.txt\n",
      "Loaded cv156_11119.txt\n",
      "Loaded cv157_29302.txt\n",
      "Loaded cv158_10914.txt\n",
      "Loaded cv159_29374.txt\n",
      "Loaded cv160_10848.txt\n",
      "Loaded cv161_12224.txt\n",
      "Loaded cv162_10977.txt\n",
      "Loaded cv163_10110.txt\n",
      "Loaded cv164_23451.txt\n",
      "Loaded cv165_2389.txt\n",
      "Loaded cv166_11959.txt\n",
      "Loaded cv167_18094.txt\n",
      "Loaded cv168_7435.txt\n",
      "Loaded cv169_24973.txt\n",
      "Loaded cv170_29808.txt\n",
      "Loaded cv171_15164.txt\n",
      "Loaded cv172_12037.txt\n",
      "Loaded cv173_4295.txt\n",
      "Loaded cv174_9735.txt\n",
      "Loaded cv175_7375.txt\n",
      "Loaded cv176_14196.txt\n",
      "Loaded cv177_10904.txt\n",
      "Loaded cv178_14380.txt\n",
      "Loaded cv179_9533.txt\n",
      "Loaded cv180_17823.txt\n",
      "Loaded cv181_16083.txt\n",
      "Loaded cv182_7791.txt\n",
      "Loaded cv183_19826.txt\n",
      "Loaded cv184_26935.txt\n",
      "Loaded cv185_28372.txt\n",
      "Loaded cv186_2396.txt\n",
      "Loaded cv187_14112.txt\n",
      "Loaded cv188_20687.txt\n",
      "Loaded cv189_24248.txt\n",
      "Loaded cv190_27176.txt\n",
      "Loaded cv191_29539.txt\n",
      "Loaded cv192_16079.txt\n",
      "Loaded cv193_5393.txt\n",
      "Loaded cv194_12855.txt\n",
      "Loaded cv195_16146.txt\n",
      "Loaded cv196_28898.txt\n",
      "Loaded cv197_29271.txt\n",
      "Loaded cv198_19313.txt\n",
      "Loaded cv199_9721.txt\n",
      "Loaded cv200_29006.txt\n",
      "Loaded cv201_7421.txt\n",
      "Loaded cv202_11382.txt\n",
      "Loaded cv203_19052.txt\n",
      "Loaded cv204_8930.txt\n",
      "Loaded cv205_9676.txt\n",
      "Loaded cv206_15893.txt\n",
      "Loaded cv207_29141.txt\n",
      "Loaded cv208_9475.txt\n",
      "Loaded cv209_28973.txt\n",
      "Loaded cv210_9557.txt\n",
      "Loaded cv211_9955.txt\n",
      "Loaded cv212_10054.txt\n",
      "Loaded cv213_20300.txt\n",
      "Loaded cv214_13285.txt\n",
      "Loaded cv215_23246.txt\n",
      "Loaded cv216_20165.txt\n",
      "Loaded cv217_28707.txt\n",
      "Loaded cv218_25651.txt\n",
      "Loaded cv219_19874.txt\n",
      "Loaded cv220_28906.txt\n",
      "Loaded cv221_27081.txt\n",
      "Loaded cv222_18720.txt\n",
      "Loaded cv223_28923.txt\n",
      "Loaded cv224_18875.txt\n",
      "Loaded cv225_29083.txt\n",
      "Loaded cv226_26692.txt\n",
      "Loaded cv227_25406.txt\n",
      "Loaded cv228_5644.txt\n",
      "Loaded cv229_15200.txt\n",
      "Loaded cv230_7913.txt\n",
      "Loaded cv231_11028.txt\n",
      "Loaded cv232_16768.txt\n",
      "Loaded cv233_17614.txt\n",
      "Loaded cv234_22123.txt\n",
      "Loaded cv235_10704.txt\n",
      "Loaded cv236_12427.txt\n",
      "Loaded cv237_20635.txt\n",
      "Loaded cv238_14285.txt\n",
      "Loaded cv239_29828.txt\n",
      "Loaded cv240_15948.txt\n",
      "Loaded cv241_24602.txt\n",
      "Loaded cv242_11354.txt\n",
      "Loaded cv243_22164.txt\n",
      "Loaded cv244_22935.txt\n",
      "Loaded cv245_8938.txt\n",
      "Loaded cv246_28668.txt\n",
      "Loaded cv247_14668.txt\n",
      "Loaded cv248_15672.txt\n",
      "Loaded cv249_12674.txt\n",
      "Loaded cv250_26462.txt\n",
      "Loaded cv251_23901.txt\n",
      "Loaded cv252_24974.txt\n",
      "Loaded cv253_10190.txt\n",
      "Loaded cv254_5870.txt\n",
      "Loaded cv255_15267.txt\n",
      "Loaded cv256_16529.txt\n",
      "Loaded cv257_11856.txt\n",
      "Loaded cv258_5627.txt\n",
      "Loaded cv259_11827.txt\n",
      "Loaded cv260_15652.txt\n",
      "Loaded cv261_11855.txt\n",
      "Loaded cv262_13812.txt\n",
      "Loaded cv263_20693.txt\n",
      "Loaded cv264_14108.txt\n",
      "Loaded cv265_11625.txt\n",
      "Loaded cv266_26644.txt\n",
      "Loaded cv267_16618.txt\n",
      "Loaded cv268_20288.txt\n",
      "Loaded cv269_23018.txt\n",
      "Loaded cv270_5873.txt\n",
      "Loaded cv271_15364.txt\n",
      "Loaded cv272_20313.txt\n",
      "Loaded cv273_28961.txt\n",
      "Loaded cv274_26379.txt\n",
      "Loaded cv275_28725.txt\n",
      "Loaded cv276_17126.txt\n",
      "Loaded cv277_20467.txt\n",
      "Loaded cv278_14533.txt\n",
      "Loaded cv279_19452.txt\n",
      "Loaded cv280_8651.txt\n",
      "Loaded cv281_24711.txt\n",
      "Loaded cv282_6833.txt\n",
      "Loaded cv283_11963.txt\n",
      "Loaded cv284_20530.txt\n",
      "Loaded cv285_18186.txt\n",
      "Loaded cv286_26156.txt\n",
      "Loaded cv287_17410.txt\n",
      "Loaded cv288_20212.txt\n",
      "Loaded cv289_6239.txt\n",
      "Loaded cv290_11981.txt\n",
      "Loaded cv291_26844.txt\n",
      "Loaded cv292_7804.txt\n",
      "Loaded cv293_29731.txt\n",
      "Loaded cv294_12695.txt\n",
      "Loaded cv295_17060.txt\n",
      "Loaded cv296_13146.txt\n",
      "Loaded cv297_10104.txt\n",
      "Loaded cv298_24487.txt\n",
      "Loaded cv299_17950.txt\n",
      "Loaded cv300_23302.txt\n",
      "Loaded cv301_13010.txt\n",
      "Loaded cv302_26481.txt\n",
      "Loaded cv303_27366.txt\n",
      "Loaded cv304_28489.txt\n",
      "Loaded cv305_9937.txt\n",
      "Loaded cv306_10859.txt\n",
      "Loaded cv307_26382.txt\n",
      "Loaded cv308_5079.txt\n",
      "Loaded cv309_23737.txt\n",
      "Loaded cv310_14568.txt\n",
      "Loaded cv311_17708.txt\n",
      "Loaded cv312_29308.txt\n",
      "Loaded cv313_19337.txt\n",
      "Loaded cv314_16095.txt\n",
      "Loaded cv315_12638.txt\n",
      "Loaded cv316_5972.txt\n",
      "Loaded cv317_25111.txt\n",
      "Loaded cv318_11146.txt\n",
      "Loaded cv319_16459.txt\n",
      "Loaded cv320_9693.txt\n",
      "Loaded cv321_14191.txt\n",
      "Loaded cv322_21820.txt\n",
      "Loaded cv323_29633.txt\n",
      "Loaded cv324_7502.txt\n",
      "Loaded cv325_18330.txt\n",
      "Loaded cv326_14777.txt\n",
      "Loaded cv327_21743.txt\n",
      "Loaded cv328_10908.txt\n",
      "Loaded cv329_29293.txt\n",
      "Loaded cv330_29675.txt\n",
      "Loaded cv331_8656.txt\n",
      "Loaded cv332_17997.txt\n",
      "Loaded cv333_9443.txt\n",
      "Loaded cv334_0074.txt\n",
      "Loaded cv335_16299.txt\n",
      "Loaded cv336_10363.txt\n",
      "Loaded cv337_29061.txt\n",
      "Loaded cv338_9183.txt\n",
      "Loaded cv339_22452.txt\n",
      "Loaded cv340_14776.txt\n",
      "Loaded cv341_25667.txt\n",
      "Loaded cv342_20917.txt\n",
      "Loaded cv343_10906.txt\n",
      "Loaded cv344_5376.txt\n",
      "Loaded cv345_9966.txt\n",
      "Loaded cv346_19198.txt\n",
      "Loaded cv347_14722.txt\n",
      "Loaded cv348_19207.txt\n",
      "Loaded cv349_15032.txt\n",
      "Loaded cv350_22139.txt\n",
      "Loaded cv351_17029.txt\n",
      "Loaded cv352_5414.txt\n",
      "Loaded cv353_19197.txt\n",
      "Loaded cv354_8573.txt\n",
      "Loaded cv355_18174.txt\n",
      "Loaded cv356_26170.txt\n",
      "Loaded cv357_14710.txt\n",
      "Loaded cv358_11557.txt\n",
      "Loaded cv359_6751.txt\n",
      "Loaded cv360_8927.txt\n",
      "Loaded cv361_28738.txt\n",
      "Loaded cv362_16985.txt\n",
      "Loaded cv363_29273.txt\n",
      "Loaded cv364_14254.txt\n",
      "Loaded cv365_12442.txt\n",
      "Loaded cv366_10709.txt\n",
      "Loaded cv367_24065.txt\n",
      "Loaded cv368_11090.txt\n",
      "Loaded cv369_14245.txt\n",
      "Loaded cv370_5338.txt\n",
      "Loaded cv371_8197.txt\n",
      "Loaded cv372_6654.txt\n",
      "Loaded cv373_21872.txt\n",
      "Loaded cv374_26455.txt\n",
      "Loaded cv375_9932.txt\n",
      "Loaded cv376_20883.txt\n",
      "Loaded cv377_8440.txt\n",
      "Loaded cv378_21982.txt\n",
      "Loaded cv379_23167.txt\n",
      "Loaded cv380_8164.txt\n",
      "Loaded cv381_21673.txt\n",
      "Loaded cv382_8393.txt\n",
      "Loaded cv383_14662.txt\n",
      "Loaded cv384_18536.txt\n",
      "Loaded cv385_29621.txt\n",
      "Loaded cv386_10229.txt\n",
      "Loaded cv387_12391.txt\n",
      "Loaded cv388_12810.txt\n",
      "Loaded cv389_9611.txt\n",
      "Loaded cv390_12187.txt\n",
      "Loaded cv391_11615.txt\n",
      "Loaded cv392_12238.txt\n",
      "Loaded cv393_29234.txt\n",
      "Loaded cv394_5311.txt\n",
      "Loaded cv395_11761.txt\n",
      "Loaded cv396_19127.txt\n",
      "Loaded cv397_28890.txt\n",
      "Loaded cv398_17047.txt\n",
      "Loaded cv399_28593.txt\n",
      "Loaded cv400_20631.txt\n",
      "Loaded cv401_13758.txt\n",
      "Loaded cv402_16097.txt\n",
      "Loaded cv403_6721.txt\n",
      "Loaded cv404_21805.txt\n",
      "Loaded cv405_21868.txt\n",
      "Loaded cv406_22199.txt\n",
      "Loaded cv407_23928.txt\n",
      "Loaded cv408_5367.txt\n",
      "Loaded cv409_29625.txt\n",
      "Loaded cv410_25624.txt\n",
      "Loaded cv411_16799.txt\n",
      "Loaded cv412_25254.txt\n",
      "Loaded cv413_7893.txt\n",
      "Loaded cv414_11161.txt\n",
      "Loaded cv415_23674.txt\n",
      "Loaded cv416_12048.txt\n",
      "Loaded cv417_14653.txt\n",
      "Loaded cv418_16562.txt\n",
      "Loaded cv419_14799.txt\n",
      "Loaded cv420_28631.txt\n",
      "Loaded cv421_9752.txt\n",
      "Loaded cv422_9632.txt\n",
      "Loaded cv423_12089.txt\n",
      "Loaded cv424_9268.txt\n",
      "Loaded cv425_8603.txt\n",
      "Loaded cv426_10976.txt\n",
      "Loaded cv427_11693.txt\n",
      "Loaded cv428_12202.txt\n",
      "Loaded cv429_7937.txt\n",
      "Loaded cv430_18662.txt\n",
      "Loaded cv431_7538.txt\n",
      "Loaded cv432_15873.txt\n",
      "Loaded cv433_10443.txt\n",
      "Loaded cv434_5641.txt\n",
      "Loaded cv435_24355.txt\n",
      "Loaded cv436_20564.txt\n",
      "Loaded cv437_24070.txt\n",
      "Loaded cv438_8500.txt\n",
      "Loaded cv439_17633.txt\n",
      "Loaded cv440_16891.txt\n",
      "Loaded cv441_15276.txt\n",
      "Loaded cv442_15499.txt\n",
      "Loaded cv443_22367.txt\n",
      "Loaded cv444_9975.txt\n",
      "Loaded cv445_26683.txt\n",
      "Loaded cv446_12209.txt\n",
      "Loaded cv447_27334.txt\n",
      "Loaded cv448_16409.txt\n",
      "Loaded cv449_9126.txt\n",
      "Loaded cv450_8319.txt\n",
      "Loaded cv451_11502.txt\n",
      "Loaded cv452_5179.txt\n",
      "Loaded cv453_10911.txt\n",
      "Loaded cv454_21961.txt\n",
      "Loaded cv455_28866.txt\n",
      "Loaded cv456_20370.txt\n",
      "Loaded cv457_19546.txt\n",
      "Loaded cv458_9000.txt\n",
      "Loaded cv459_21834.txt\n",
      "Loaded cv460_11723.txt\n",
      "Loaded cv461_21124.txt\n",
      "Loaded cv462_20788.txt\n",
      "Loaded cv463_10846.txt\n",
      "Loaded cv464_17076.txt\n",
      "Loaded cv465_23401.txt\n",
      "Loaded cv466_20092.txt\n",
      "Loaded cv467_26610.txt\n",
      "Loaded cv468_16844.txt\n",
      "Loaded cv469_21998.txt\n",
      "Loaded cv470_17444.txt\n",
      "Loaded cv471_18405.txt\n",
      "Loaded cv472_29140.txt\n",
      "Loaded cv473_7869.txt\n",
      "Loaded cv474_10682.txt\n",
      "Loaded cv475_22978.txt\n",
      "Loaded cv476_18402.txt\n",
      "Loaded cv477_23530.txt\n",
      "Loaded cv478_15921.txt\n",
      "Loaded cv479_5450.txt\n",
      "Loaded cv480_21195.txt\n",
      "Loaded cv481_7930.txt\n",
      "Loaded cv482_11233.txt\n",
      "Loaded cv483_18103.txt\n",
      "Loaded cv484_26169.txt\n",
      "Loaded cv485_26879.txt\n",
      "Loaded cv486_9788.txt\n",
      "Loaded cv487_11058.txt\n",
      "Loaded cv488_21453.txt\n",
      "Loaded cv489_19046.txt\n",
      "Loaded cv490_18986.txt\n",
      "Loaded cv491_12992.txt\n",
      "Loaded cv492_19370.txt\n",
      "Loaded cv493_14135.txt\n",
      "Loaded cv494_18689.txt\n",
      "Loaded cv495_16121.txt\n",
      "Loaded cv496_11185.txt\n",
      "Loaded cv497_27086.txt\n",
      "Loaded cv498_9288.txt\n",
      "Loaded cv499_11407.txt\n",
      "Loaded cv500_10722.txt\n",
      "Loaded cv501_12675.txt\n",
      "Loaded cv502_10970.txt\n",
      "Loaded cv503_11196.txt\n",
      "Loaded cv504_29120.txt\n",
      "Loaded cv505_12926.txt\n",
      "Loaded cv506_17521.txt\n",
      "Loaded cv507_9509.txt\n",
      "Loaded cv508_17742.txt\n",
      "Loaded cv509_17354.txt\n",
      "Loaded cv510_24758.txt\n",
      "Loaded cv511_10360.txt\n",
      "Loaded cv512_17618.txt\n",
      "Loaded cv513_7236.txt\n",
      "Loaded cv514_12173.txt\n",
      "Loaded cv515_18484.txt\n",
      "Loaded cv516_12117.txt\n",
      "Loaded cv517_20616.txt\n",
      "Loaded cv518_14798.txt\n",
      "Loaded cv519_16239.txt\n",
      "Loaded cv520_13297.txt\n",
      "Loaded cv521_1730.txt\n",
      "Loaded cv522_5418.txt\n",
      "Loaded cv523_18285.txt\n",
      "Loaded cv524_24885.txt\n",
      "Loaded cv525_17930.txt\n",
      "Loaded cv526_12868.txt\n",
      "Loaded cv527_10338.txt\n",
      "Loaded cv528_11669.txt\n",
      "Loaded cv529_10972.txt\n",
      "Loaded cv530_17949.txt\n",
      "Loaded cv531_26838.txt\n",
      "Loaded cv532_6495.txt\n",
      "Loaded cv533_9843.txt\n",
      "Loaded cv534_15683.txt\n",
      "Loaded cv535_21183.txt\n",
      "Loaded cv536_27221.txt\n",
      "Loaded cv537_13516.txt\n",
      "Loaded cv538_28485.txt\n",
      "Loaded cv539_21865.txt\n",
      "Loaded cv540_3092.txt\n",
      "Loaded cv541_28683.txt\n",
      "Loaded cv542_20359.txt\n",
      "Loaded cv543_5107.txt\n",
      "Loaded cv544_5301.txt\n",
      "Loaded cv545_12848.txt\n",
      "Loaded cv546_12723.txt\n",
      "Loaded cv547_18043.txt\n",
      "Loaded cv548_18944.txt\n",
      "Loaded cv549_22771.txt\n",
      "Loaded cv550_23226.txt\n",
      "Loaded cv551_11214.txt\n",
      "Loaded cv552_0150.txt\n",
      "Loaded cv553_26965.txt\n",
      "Loaded cv554_14678.txt\n",
      "Loaded cv555_25047.txt\n",
      "Loaded cv556_16563.txt\n",
      "Loaded cv557_12237.txt\n",
      "Loaded cv558_29376.txt\n",
      "Loaded cv559_0057.txt\n",
      "Loaded cv560_18608.txt\n",
      "Loaded cv561_9484.txt\n",
      "Loaded cv562_10847.txt\n",
      "Loaded cv563_18610.txt\n",
      "Loaded cv564_12011.txt\n",
      "Loaded cv565_29403.txt\n",
      "Loaded cv566_8967.txt\n",
      "Loaded cv567_29420.txt\n",
      "Loaded cv568_17065.txt\n",
      "Loaded cv569_26750.txt\n",
      "Loaded cv570_28960.txt\n",
      "Loaded cv571_29292.txt\n",
      "Loaded cv572_20053.txt\n",
      "Loaded cv573_29384.txt\n",
      "Loaded cv574_23191.txt\n",
      "Loaded cv575_22598.txt\n",
      "Loaded cv576_15688.txt\n",
      "Loaded cv577_28220.txt\n",
      "Loaded cv578_16825.txt\n",
      "Loaded cv579_12542.txt\n",
      "Loaded cv580_15681.txt\n",
      "Loaded cv581_20790.txt\n",
      "Loaded cv582_6678.txt\n",
      "Loaded cv583_29465.txt\n",
      "Loaded cv584_29549.txt\n",
      "Loaded cv585_23576.txt\n",
      "Loaded cv586_8048.txt\n",
      "Loaded cv587_20532.txt\n",
      "Loaded cv588_14467.txt\n",
      "Loaded cv589_12853.txt\n",
      "Loaded cv590_20712.txt\n",
      "Loaded cv591_24887.txt\n",
      "Loaded cv592_23391.txt\n",
      "Loaded cv593_11931.txt\n",
      "Loaded cv594_11945.txt\n",
      "Loaded cv595_26420.txt\n",
      "Loaded cv596_4367.txt\n",
      "Loaded cv597_26744.txt\n",
      "Loaded cv598_18184.txt\n",
      "Loaded cv599_22197.txt\n",
      "Loaded cv600_25043.txt\n",
      "Loaded cv601_24759.txt\n",
      "Loaded cv602_8830.txt\n",
      "Loaded cv603_18885.txt\n",
      "Loaded cv604_23339.txt\n",
      "Loaded cv605_12730.txt\n",
      "Loaded cv606_17672.txt\n",
      "Loaded cv607_8235.txt\n",
      "Loaded cv608_24647.txt\n",
      "Loaded cv609_25038.txt\n",
      "Loaded cv610_24153.txt\n",
      "Loaded cv611_2253.txt\n",
      "Loaded cv612_5396.txt\n",
      "Loaded cv613_23104.txt\n",
      "Loaded cv614_11320.txt\n",
      "Loaded cv615_15734.txt\n",
      "Loaded cv616_29187.txt\n",
      "Loaded cv617_9561.txt\n",
      "Loaded cv618_9469.txt\n",
      "Loaded cv619_13677.txt\n",
      "Loaded cv620_2556.txt\n",
      "Loaded cv621_15984.txt\n",
      "Loaded cv622_8583.txt\n",
      "Loaded cv623_16988.txt\n",
      "Loaded cv624_11601.txt\n",
      "Loaded cv625_13518.txt\n",
      "Loaded cv626_7907.txt\n",
      "Loaded cv627_12603.txt\n",
      "Loaded cv628_20758.txt\n",
      "Loaded cv629_16604.txt\n",
      "Loaded cv630_10152.txt\n",
      "Loaded cv631_4782.txt\n",
      "Loaded cv632_9704.txt\n",
      "Loaded cv633_29730.txt\n",
      "Loaded cv634_11989.txt\n",
      "Loaded cv635_0984.txt\n",
      "Loaded cv636_16954.txt\n",
      "Loaded cv637_13682.txt\n",
      "Loaded cv638_29394.txt\n",
      "Loaded cv639_10797.txt\n",
      "Loaded cv640_5380.txt\n",
      "Loaded cv641_13412.txt\n",
      "Loaded cv642_29788.txt\n",
      "Loaded cv643_29282.txt\n",
      "Loaded cv644_18551.txt\n",
      "Loaded cv645_17078.txt\n",
      "Loaded cv646_16817.txt\n",
      "Loaded cv647_15275.txt\n",
      "Loaded cv648_17277.txt\n",
      "Loaded cv649_13947.txt\n",
      "Loaded cv650_15974.txt\n",
      "Loaded cv651_11120.txt\n",
      "Loaded cv652_15653.txt\n",
      "Loaded cv653_2107.txt\n",
      "Loaded cv654_19345.txt\n",
      "Loaded cv655_12055.txt\n",
      "Loaded cv656_25395.txt\n",
      "Loaded cv657_25835.txt\n",
      "Loaded cv658_11186.txt\n",
      "Loaded cv659_21483.txt\n",
      "Loaded cv660_23140.txt\n",
      "Loaded cv661_25780.txt\n",
      "Loaded cv662_14791.txt\n",
      "Loaded cv663_14484.txt\n",
      "Loaded cv664_4264.txt\n",
      "Loaded cv665_29386.txt\n",
      "Loaded cv666_20301.txt\n",
      "Loaded cv667_19672.txt\n",
      "Loaded cv668_18848.txt\n",
      "Loaded cv669_24318.txt\n",
      "Loaded cv670_2666.txt\n",
      "Loaded cv671_5164.txt\n",
      "Loaded cv672_27988.txt\n",
      "Loaded cv673_25874.txt\n",
      "Loaded cv674_11593.txt\n",
      "Loaded cv675_22871.txt\n",
      "Loaded cv676_22202.txt\n",
      "Loaded cv677_18938.txt\n",
      "Loaded cv678_14887.txt\n",
      "Loaded cv679_28221.txt\n",
      "Loaded cv680_10533.txt\n",
      "Loaded cv681_9744.txt\n",
      "Loaded cv682_17947.txt\n",
      "Loaded cv683_13047.txt\n",
      "Loaded cv684_12727.txt\n",
      "Loaded cv685_5710.txt\n",
      "Loaded cv686_15553.txt\n",
      "Loaded cv687_22207.txt\n",
      "Loaded cv688_7884.txt\n",
      "Loaded cv689_13701.txt\n",
      "Loaded cv690_5425.txt\n",
      "Loaded cv691_5090.txt\n",
      "Loaded cv692_17026.txt\n",
      "Loaded cv693_19147.txt\n",
      "Loaded cv694_4526.txt\n",
      "Loaded cv695_22268.txt\n",
      "Loaded cv696_29619.txt\n",
      "Loaded cv697_12106.txt\n",
      "Loaded cv698_16930.txt\n",
      "Loaded cv699_7773.txt\n",
      "Loaded cv700_23163.txt\n",
      "Loaded cv701_15880.txt\n",
      "Loaded cv702_12371.txt\n",
      "Loaded cv703_17948.txt\n",
      "Loaded cv704_17622.txt\n",
      "Loaded cv705_11973.txt\n",
      "Loaded cv706_25883.txt\n",
      "Loaded cv707_11421.txt\n",
      "Loaded cv708_28539.txt\n",
      "Loaded cv709_11173.txt\n",
      "Loaded cv710_23745.txt\n",
      "Loaded cv711_12687.txt\n",
      "Loaded cv712_24217.txt\n",
      "Loaded cv713_29002.txt\n",
      "Loaded cv714_19704.txt\n",
      "Loaded cv715_19246.txt\n",
      "Loaded cv716_11153.txt\n",
      "Loaded cv717_17472.txt\n",
      "Loaded cv718_12227.txt\n",
      "Loaded cv719_5581.txt\n",
      "Loaded cv720_5383.txt\n",
      "Loaded cv721_28993.txt\n",
      "Loaded cv722_7571.txt\n",
      "Loaded cv723_9002.txt\n",
      "Loaded cv724_15265.txt\n",
      "Loaded cv725_10266.txt\n",
      "Loaded cv726_4365.txt\n",
      "Loaded cv727_5006.txt\n",
      "Loaded cv728_17931.txt\n",
      "Loaded cv729_10475.txt\n",
      "Loaded cv730_10729.txt\n",
      "Loaded cv731_3968.txt\n",
      "Loaded cv732_13092.txt\n",
      "Loaded cv733_9891.txt\n",
      "Loaded cv734_22821.txt\n",
      "Loaded cv735_20218.txt\n",
      "Loaded cv736_24947.txt\n",
      "Loaded cv737_28733.txt\n",
      "Loaded cv738_10287.txt\n",
      "Loaded cv739_12179.txt\n",
      "Loaded cv740_13643.txt\n",
      "Loaded cv741_12765.txt\n",
      "Loaded cv742_8279.txt\n",
      "Loaded cv743_17023.txt\n",
      "Loaded cv744_10091.txt\n",
      "Loaded cv745_14009.txt\n",
      "Loaded cv746_10471.txt\n",
      "Loaded cv747_18189.txt\n",
      "Loaded cv748_14044.txt\n",
      "Loaded cv749_18960.txt\n",
      "Loaded cv750_10606.txt\n",
      "Loaded cv751_17208.txt\n",
      "Loaded cv752_25330.txt\n",
      "Loaded cv753_11812.txt\n",
      "Loaded cv754_7709.txt\n",
      "Loaded cv755_24881.txt\n",
      "Loaded cv756_23676.txt\n",
      "Loaded cv757_10668.txt\n",
      "Loaded cv758_9740.txt\n",
      "Loaded cv759_15091.txt\n",
      "Loaded cv760_8977.txt\n",
      "Loaded cv761_13769.txt\n",
      "Loaded cv762_15604.txt\n",
      "Loaded cv763_16486.txt\n",
      "Loaded cv764_12701.txt\n",
      "Loaded cv765_20429.txt\n",
      "Loaded cv766_7983.txt\n",
      "Loaded cv767_15673.txt\n",
      "Loaded cv768_12709.txt\n",
      "Loaded cv769_8565.txt\n",
      "Loaded cv770_11061.txt\n",
      "Loaded cv771_28466.txt\n",
      "Loaded cv772_12971.txt\n",
      "Loaded cv773_20264.txt\n",
      "Loaded cv774_15488.txt\n",
      "Loaded cv775_17966.txt\n",
      "Loaded cv776_21934.txt\n",
      "Loaded cv777_10247.txt\n",
      "Loaded cv778_18629.txt\n",
      "Loaded cv779_18989.txt\n",
      "Loaded cv780_8467.txt\n",
      "Loaded cv781_5358.txt\n",
      "Loaded cv782_21078.txt\n",
      "Loaded cv783_14724.txt\n",
      "Loaded cv784_16077.txt\n",
      "Loaded cv785_23748.txt\n",
      "Loaded cv786_23608.txt\n",
      "Loaded cv787_15277.txt\n",
      "Loaded cv788_26409.txt\n",
      "Loaded cv789_12991.txt\n",
      "Loaded cv790_16202.txt\n",
      "Loaded cv791_17995.txt\n",
      "Loaded cv792_3257.txt\n",
      "Loaded cv793_15235.txt\n",
      "Loaded cv794_17353.txt\n",
      "Loaded cv795_10291.txt\n",
      "Loaded cv796_17243.txt\n",
      "Loaded cv797_7245.txt\n",
      "Loaded cv798_24779.txt\n",
      "Loaded cv799_19812.txt\n",
      "Loaded cv800_13494.txt\n",
      "Loaded cv801_26335.txt\n",
      "Loaded cv802_28381.txt\n",
      "Loaded cv803_8584.txt\n",
      "Loaded cv804_11763.txt\n",
      "Loaded cv805_21128.txt\n",
      "Loaded cv806_9405.txt\n",
      "Loaded cv807_23024.txt\n",
      "Loaded cv808_13773.txt\n",
      "Loaded cv809_5012.txt\n",
      "Loaded cv810_13660.txt\n",
      "Loaded cv811_22646.txt\n",
      "Loaded cv812_19051.txt\n",
      "Loaded cv813_6649.txt\n",
      "Loaded cv814_20316.txt\n",
      "Loaded cv815_23466.txt\n",
      "Loaded cv816_15257.txt\n",
      "Loaded cv817_3675.txt\n",
      "Loaded cv818_10698.txt\n",
      "Loaded cv819_9567.txt\n",
      "Loaded cv820_24157.txt\n",
      "Loaded cv821_29283.txt\n",
      "Loaded cv822_21545.txt\n",
      "Loaded cv823_17055.txt\n",
      "Loaded cv824_9335.txt\n",
      "Loaded cv825_5168.txt\n",
      "Loaded cv826_12761.txt\n",
      "Loaded cv827_19479.txt\n",
      "Loaded cv828_21392.txt\n",
      "Loaded cv829_21725.txt\n",
      "Loaded cv830_5778.txt\n",
      "Loaded cv831_16325.txt\n",
      "Loaded cv832_24713.txt\n",
      "Loaded cv833_11961.txt\n",
      "Loaded cv834_23192.txt\n",
      "Loaded cv835_20531.txt\n",
      "Loaded cv836_14311.txt\n",
      "Loaded cv837_27232.txt\n",
      "Loaded cv838_25886.txt\n",
      "Loaded cv839_22807.txt\n",
      "Loaded cv840_18033.txt\n",
      "Loaded cv841_3367.txt\n",
      "Loaded cv842_5702.txt\n",
      "Loaded cv843_17054.txt\n",
      "Loaded cv844_13890.txt\n",
      "Loaded cv845_15886.txt\n",
      "Loaded cv846_29359.txt\n",
      "Loaded cv847_20855.txt\n",
      "Loaded cv848_10061.txt\n",
      "Loaded cv849_17215.txt\n",
      "Loaded cv850_18185.txt\n",
      "Loaded cv851_21895.txt\n",
      "Loaded cv852_27512.txt\n",
      "Loaded cv853_29119.txt\n",
      "Loaded cv854_18955.txt\n",
      "Loaded cv855_22134.txt\n",
      "Loaded cv856_28882.txt\n",
      "Loaded cv857_17527.txt\n",
      "Loaded cv858_20266.txt\n",
      "Loaded cv859_15689.txt\n",
      "Loaded cv860_15520.txt\n",
      "Loaded cv861_12809.txt\n",
      "Loaded cv862_15924.txt\n",
      "Loaded cv863_7912.txt\n",
      "Loaded cv864_3087.txt\n",
      "Loaded cv865_28796.txt\n",
      "Loaded cv866_29447.txt\n",
      "Loaded cv867_18362.txt\n",
      "Loaded cv868_12799.txt\n",
      "Loaded cv869_24782.txt\n",
      "Loaded cv870_18090.txt\n",
      "Loaded cv871_25971.txt\n",
      "Loaded cv872_13710.txt\n",
      "Loaded cv873_19937.txt\n",
      "Loaded cv874_12182.txt\n",
      "Loaded cv875_5622.txt\n",
      "Loaded cv876_9633.txt\n",
      "Loaded cv877_29132.txt\n",
      "Loaded cv878_17204.txt\n",
      "Loaded cv879_16585.txt\n",
      "Loaded cv880_29629.txt\n",
      "Loaded cv881_14767.txt\n",
      "Loaded cv882_10042.txt\n",
      "Loaded cv883_27621.txt\n",
      "Loaded cv884_15230.txt\n",
      "Loaded cv885_13390.txt\n",
      "Loaded cv886_19210.txt\n",
      "Loaded cv887_5306.txt\n",
      "Loaded cv888_25678.txt\n",
      "Loaded cv889_22670.txt\n",
      "Loaded cv890_3515.txt\n",
      "Loaded cv891_6035.txt\n",
      "Loaded cv892_18788.txt\n",
      "Loaded cv893_26731.txt\n",
      "Loaded cv894_22140.txt\n",
      "Loaded cv895_22200.txt\n",
      "Loaded cv896_17819.txt\n",
      "Loaded cv897_11703.txt\n",
      "Loaded cv898_1576.txt\n",
      "Loaded cv899_17812.txt\n",
      "Loaded cv900_10800.txt\n",
      "Loaded cv901_11934.txt\n",
      "Loaded cv902_13217.txt\n",
      "Loaded cv903_18981.txt\n",
      "Loaded cv904_25663.txt\n",
      "Loaded cv905_28965.txt\n",
      "Loaded cv906_12332.txt\n",
      "Loaded cv907_3193.txt\n",
      "Loaded cv908_17779.txt\n",
      "Loaded cv909_9973.txt\n",
      "Loaded cv910_21930.txt\n",
      "Loaded cv911_21695.txt\n",
      "Loaded cv912_5562.txt\n",
      "Loaded cv913_29127.txt\n",
      "Loaded cv914_2856.txt\n",
      "Loaded cv915_9342.txt\n",
      "Loaded cv916_17034.txt\n",
      "Loaded cv917_29484.txt\n",
      "Loaded cv918_27080.txt\n",
      "Loaded cv919_18155.txt\n",
      "Loaded cv920_29423.txt\n",
      "Loaded cv921_13988.txt\n",
      "Loaded cv922_10185.txt\n",
      "Loaded cv923_11951.txt\n",
      "Loaded cv924_29397.txt\n",
      "Loaded cv925_9459.txt\n",
      "Loaded cv926_18471.txt\n",
      "Loaded cv927_11471.txt\n",
      "Loaded cv928_9478.txt\n",
      "Loaded cv929_1841.txt\n",
      "Loaded cv930_14949.txt\n",
      "Loaded cv931_18783.txt\n",
      "Loaded cv932_14854.txt\n",
      "Loaded cv933_24953.txt\n",
      "Loaded cv934_20426.txt\n",
      "Loaded cv935_24977.txt\n",
      "Loaded cv936_17473.txt\n",
      "Loaded cv937_9816.txt\n",
      "Loaded cv938_10706.txt\n",
      "Loaded cv939_11247.txt\n",
      "Loaded cv940_18935.txt\n",
      "Loaded cv941_10718.txt\n",
      "Loaded cv942_18509.txt\n",
      "Loaded cv943_23547.txt\n",
      "Loaded cv944_15042.txt\n",
      "Loaded cv945_13012.txt\n",
      "Loaded cv946_20084.txt\n",
      "Loaded cv947_11316.txt\n",
      "Loaded cv948_25870.txt\n",
      "Loaded cv949_21565.txt\n",
      "Loaded cv950_13478.txt\n",
      "Loaded cv951_11816.txt\n",
      "Loaded cv952_26375.txt\n",
      "Loaded cv953_7078.txt\n",
      "Loaded cv954_19932.txt\n",
      "Loaded cv955_26154.txt\n",
      "Loaded cv956_12547.txt\n",
      "Loaded cv957_9059.txt\n",
      "Loaded cv958_13020.txt\n",
      "Loaded cv959_16218.txt\n",
      "Loaded cv960_28877.txt\n",
      "Loaded cv961_5578.txt\n",
      "Loaded cv962_9813.txt\n",
      "Loaded cv963_7208.txt\n",
      "Loaded cv964_5794.txt\n",
      "Loaded cv965_26688.txt\n",
      "Loaded cv966_28671.txt\n",
      "Loaded cv967_5626.txt\n",
      "Loaded cv968_25413.txt\n",
      "Loaded cv969_14760.txt\n",
      "Loaded cv970_19532.txt\n",
      "Loaded cv971_11790.txt\n",
      "Loaded cv972_26837.txt\n",
      "Loaded cv973_10171.txt\n",
      "Loaded cv974_24303.txt\n",
      "Loaded cv975_11920.txt\n",
      "Loaded cv976_10724.txt\n",
      "Loaded cv977_4776.txt\n",
      "Loaded cv978_22192.txt\n",
      "Loaded cv979_2029.txt\n",
      "Loaded cv980_11851.txt\n",
      "Loaded cv981_16679.txt\n",
      "Loaded cv982_22209.txt\n",
      "Loaded cv983_24219.txt\n",
      "Loaded cv984_14006.txt\n",
      "Loaded cv985_5964.txt\n",
      "Loaded cv986_15092.txt\n",
      "Loaded cv987_7394.txt\n",
      "Loaded cv988_20168.txt\n",
      "Loaded cv989_17297.txt\n",
      "Loaded cv990_12443.txt\n",
      "Loaded cv991_19973.txt\n",
      "Loaded cv992_12806.txt\n",
      "Loaded cv993_29565.txt\n",
      "Loaded cv994_13229.txt\n",
      "Loaded cv995_23113.txt\n",
      "Loaded cv996_12447.txt\n",
      "Loaded cv997_5152.txt\n",
      "Loaded cv998_15691.txt\n",
      "Loaded cv999_14636.txt\n"
     ]
    }
   ],
   "source": [
    "from os import listdir\n",
    "\n",
    "# load doc into memory\n",
    "def load_doc(filename):\n",
    "\t# open the file as read only\n",
    "\tfile = open(filename, 'r')\n",
    "\t# read all text\n",
    "\ttext = file.read()\n",
    "\t# close the file\n",
    "\tfile.close()\n",
    "\treturn text\n",
    "\n",
    "# load all docs in a directory\n",
    "def process_docs(directory):\n",
    "\t# walk through all files in the folder\n",
    "\tfor filename in listdir(directory):\n",
    "\t\t# skip files that do not have the right extension\n",
    "\t\tif not filename.endswith(\".txt\"):\n",
    "\t\t\tnext\n",
    "\t\t# create the full path of the file to open\n",
    "\t\tpath = directory + '/' + filename\n",
    "\t\t# load document\n",
    "\t\tdoc = load_doc(path)\n",
    "\t\tprint('Loaded %s' % filename)\n",
    "\n",
    "# specify directory to load\n",
    "directory = 'txt_sentoken/neg'\n",
    "process_docs(directory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### load and split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['plot', ':', 'two', 'teen', 'couples', 'go', 'to', 'a', 'church', 'party', ',', 'drink', 'and', 'then', 'drive', '.', 'they', 'get', 'into', 'an', 'accident', '.', 'one', 'of', 'the', 'guys', 'dies', ',', 'but', 'his', 'girlfriend', 'continues', 'to', 'see', 'him', 'in', 'her', 'life', ',', 'and', 'has', 'nightmares', '.', \"what's\", 'the', 'deal', '?', 'watch', 'the', 'movie', 'and', '\"', 'sorta', '\"', 'find', 'out', '.', '.', '.', 'critique', ':', 'a', 'mind-fuck', 'movie', 'for', 'the', 'teen', 'generation', 'that', 'touches', 'on', 'a', 'very', 'cool', 'idea', ',', 'but', 'presents', 'it', 'in', 'a', 'very', 'bad', 'package', '.', 'which', 'is', 'what', 'makes', 'this', 'review', 'an', 'even', 'harder', 'one', 'to', 'write', ',', 'since', 'i', 'generally', 'applaud', 'films', 'which', 'attempt', 'to', 'break', 'the', 'mold', ',', 'mess', 'with', 'your', 'head', 'and', 'such', '(', 'lost', 'highway', '&', 'memento', ')', ',', 'but', 'there', 'are', 'good', 'and', 'bad', 'ways', 'of', 'making', 'all', 'types', 'of', 'films', ',', 'and', 'these', 'folks', 'just', \"didn't\", 'snag', 'this', 'one', 'correctly', '.', 'they', 'seem', 'to', 'have', 'taken', 'this', 'pretty', 'neat', 'concept', ',', 'but', 'executed', 'it', 'terribly', '.', 'so', 'what', 'are', 'the', 'problems', 'with', 'the', 'movie', '?', 'well', ',', 'its', 'main', 'problem', 'is', 'that', \"it's\", 'simply', 'too', 'jumbled', '.', 'it', 'starts', 'off', '\"', 'normal', '\"', 'but', 'then', 'downshifts', 'into', 'this', '\"', 'fantasy', '\"', 'world', 'in', 'which', 'you', ',', 'as', 'an', 'audience', 'member', ',', 'have', 'no', 'idea', \"what's\", 'going', 'on', '.', 'there', 'are', 'dreams', ',', 'there', 'are', 'characters', 'coming', 'back', 'from', 'the', 'dead', ',', 'there', 'are', 'others', 'who', 'look', 'like', 'the', 'dead', ',', 'there', 'are', 'strange', 'apparitions', ',', 'there', 'are', 'disappearances', ',', 'there', 'are', 'a', 'looooot', 'of', 'chase', 'scenes', ',', 'there', 'are', 'tons', 'of', 'weird', 'things', 'that', 'happen', ',', 'and', 'most', 'of', 'it', 'is', 'simply', 'not', 'explained', '.', 'now', 'i', 'personally', \"don't\", 'mind', 'trying', 'to', 'unravel', 'a', 'film', 'every', 'now', 'and', 'then', ',', 'but', 'when', 'all', 'it', 'does', 'is', 'give', 'me', 'the', 'same', 'clue', 'over', 'and', 'over', 'again', ',', 'i', 'get', 'kind', 'of', 'fed', 'up', 'after', 'a', 'while', ',', 'which', 'is', 'this', \"film's\", 'biggest', 'problem', '.', \"it's\", 'obviously', 'got', 'this', 'big', 'secret', 'to', 'hide', ',', 'but', 'it', 'seems', 'to', 'want', 'to', 'hide', 'it', 'completely', 'until', 'its', 'final', 'five', 'minutes', '.', 'and', 'do', 'they', 'make', 'things', 'entertaining', ',', 'thrilling', 'or', 'even', 'engaging', ',', 'in', 'the', 'meantime', '?', 'not', 'really', '.', 'the', 'sad', 'part', 'is', 'that', 'the', 'arrow', 'and', 'i', 'both', 'dig', 'on', 'flicks', 'like', 'this', ',', 'so', 'we', 'actually', 'figured', 'most', 'of', 'it', 'out', 'by', 'the', 'half-way', 'point', ',', 'so', 'all', 'of', 'the', 'strangeness', 'after', 'that', 'did', 'start', 'to', 'make', 'a', 'little', 'bit', 'of', 'sense', ',', 'but', 'it', 'still', \"didn't\", 'the', 'make', 'the', 'film', 'all', 'that', 'more', 'entertaining', '.', 'i', 'guess', 'the', 'bottom', 'line', 'with', 'movies', 'like', 'this', 'is', 'that', 'you', 'should', 'always', 'make', 'sure', 'that', 'the', 'audience', 'is', '\"', 'into', 'it', '\"', 'even', 'before', 'they', 'are', 'given', 'the', 'secret', 'password', 'to', 'enter', 'your', 'world', 'of', 'understanding', '.', 'i', 'mean', ',', 'showing', 'melissa', 'sagemiller', 'running', 'away', 'from', 'visions', 'for', 'about', '20', 'minutes', 'throughout', 'the', 'movie', 'is', 'just', 'plain', 'lazy', '!', '!', 'okay', ',', 'we', 'get', 'it', '.', '.', '.', 'there', 'are', 'people', 'chasing', 'her', 'and', 'we', \"don't\", 'know', 'who', 'they', 'are', '.', 'do', 'we', 'really', 'need', 'to', 'see', 'it', 'over', 'and', 'over', 'again', '?', 'how', 'about', 'giving', 'us', 'different', 'scenes', 'offering', 'further', 'insight', 'into', 'all', 'of', 'the', 'strangeness', 'going', 'down', 'in', 'the', 'movie', '?', 'apparently', ',', 'the', 'studio', 'took', 'this', 'film', 'away', 'from', 'its', 'director', 'and', 'chopped', 'it', 'up', 'themselves', ',', 'and', 'it', 'shows', '.', 'there', \"might've\", 'been', 'a', 'pretty', 'decent', 'teen', 'mind-fuck', 'movie', 'in', 'here', 'somewhere', ',', 'but', 'i', 'guess', '\"', 'the', 'suits', '\"', 'decided', 'that', 'turning', 'it', 'into', 'a', 'music', 'video', 'with', 'little', 'edge', ',', 'would', 'make', 'more', 'sense', '.', 'the', 'actors', 'are', 'pretty', 'good', 'for', 'the', 'most', 'part', ',', 'although', 'wes', 'bentley', 'just', 'seemed', 'to', 'be', 'playing', 'the', 'exact', 'same', 'character', 'that', 'he', 'did', 'in', 'american', 'beauty', ',', 'only', 'in', 'a', 'new', 'neighborhood', '.', 'but', 'my', 'biggest', 'kudos', 'go', 'out', 'to', 'sagemiller', ',', 'who', 'holds', 'her', 'own', 'throughout', 'the', 'entire', 'film', ',', 'and', 'actually', 'has', 'you', 'feeling', 'her', \"character's\", 'unraveling', '.', 'overall', ',', 'the', 'film', \"doesn't\", 'stick', 'because', 'it', \"doesn't\", 'entertain', ',', \"it's\", 'confusing', ',', 'it', 'rarely', 'excites', 'and', 'it', 'feels', 'pretty', 'redundant', 'for', 'most', 'of', 'its', 'runtime', ',', 'despite', 'a', 'pretty', 'cool', 'ending', 'and', 'explanation', 'to', 'all', 'of', 'the', 'craziness', 'that', 'came', 'before', 'it', '.', 'oh', ',', 'and', 'by', 'the', 'way', ',', 'this', 'is', 'not', 'a', 'horror', 'or', 'teen', 'slasher', 'flick', '.', '.', '.', \"it's\", 'just', 'packaged', 'to', 'look', 'that', 'way', 'because', 'someone', 'is', 'apparently', 'assuming', 'that', 'the', 'genre', 'is', 'still', 'hot', 'with', 'the', 'kids', '.', 'it', 'also', 'wrapped', 'production', 'two', 'years', 'ago', 'and', 'has', 'been', 'sitting', 'on', 'the', 'shelves', 'ever', 'since', '.', 'whatever', '.', '.', '.', 'skip', 'it', '!', \"where's\", 'joblo', 'coming', 'from', '?', 'a', 'nightmare', 'of', 'elm', 'street', '3', '(', '7/10', ')', '-', 'blair', 'witch', '2', '(', '7/10', ')', '-', 'the', 'crow', '(', '9/10', ')', '-', 'the', 'crow', ':', 'salvation', '(', '4/10', ')', '-', 'lost', 'highway', '(', '10/10', ')', '-', 'memento', '(', '10/10', ')', '-', 'the', 'others', '(', '9/10', ')', '-', 'stir', 'of', 'echoes', '(', '8/10', ')']\n"
     ]
    }
   ],
   "source": [
    "# load doc into memory\n",
    "def load_doc(filename):\n",
    "\t# open the file as read only\n",
    "\tfile = open(filename, 'r')\n",
    "\t# read all text\n",
    "\ttext = file.read()\n",
    "\t# close the file\n",
    "\tfile.close()\n",
    "\treturn text\n",
    "\n",
    "# load the document\n",
    "filename = 'cv000_29416.txt'\n",
    "text = load_doc(filename)\n",
    "# split into tokens by white space\n",
    "tokens = text.split()\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['plot', 'two', 'teen', 'couples', 'go', 'church', 'party', 'drink', 'drive', 'get', 'accident', 'one', 'guys', 'dies', 'girlfriend', 'continues', 'see', 'life', 'nightmares', 'whats', 'deal', 'watch', 'movie', 'sorta', 'find', 'critique', 'mindfuck', 'movie', 'teen', 'generation', 'touches', 'cool', 'idea', 'presents', 'bad', 'package', 'makes', 'review', 'even', 'harder', 'one', 'write', 'since', 'generally', 'applaud', 'films', 'attempt', 'break', 'mold', 'mess', 'head', 'lost', 'highway', 'memento', 'good', 'bad', 'ways', 'making', 'types', 'films', 'folks', 'didnt', 'snag', 'one', 'correctly', 'seem', 'taken', 'pretty', 'neat', 'concept', 'executed', 'terribly', 'problems', 'movie', 'well', 'main', 'problem', 'simply', 'jumbled', 'starts', 'normal', 'downshifts', 'fantasy', 'world', 'audience', 'member', 'idea', 'whats', 'going', 'dreams', 'characters', 'coming', 'back', 'dead', 'others', 'look', 'like', 'dead', 'strange', 'apparitions', 'disappearances', 'looooot', 'chase', 'scenes', 'tons', 'weird', 'things', 'happen', 'simply', 'explained', 'personally', 'dont', 'mind', 'trying', 'unravel', 'film', 'every', 'give', 'clue', 'get', 'kind', 'fed', 'films', 'biggest', 'problem', 'obviously', 'got', 'big', 'secret', 'hide', 'seems', 'want', 'hide', 'completely', 'final', 'five', 'minutes', 'make', 'things', 'entertaining', 'thrilling', 'even', 'engaging', 'meantime', 'really', 'sad', 'part', 'arrow', 'dig', 'flicks', 'like', 'actually', 'figured', 'halfway', 'point', 'strangeness', 'start', 'make', 'little', 'bit', 'sense', 'still', 'didnt', 'make', 'film', 'entertaining', 'guess', 'bottom', 'line', 'movies', 'like', 'always', 'make', 'sure', 'audience', 'even', 'given', 'secret', 'password', 'enter', 'world', 'understanding', 'mean', 'showing', 'melissa', 'sagemiller', 'running', 'away', 'visions', 'minutes', 'throughout', 'movie', 'plain', 'lazy', 'okay', 'get', 'people', 'chasing', 'dont', 'know', 'really', 'need', 'see', 'giving', 'us', 'different', 'scenes', 'offering', 'insight', 'strangeness', 'going', 'movie', 'apparently', 'studio', 'took', 'film', 'away', 'director', 'chopped', 'shows', 'mightve', 'pretty', 'decent', 'teen', 'mindfuck', 'movie', 'somewhere', 'guess', 'suits', 'decided', 'turning', 'music', 'video', 'little', 'edge', 'would', 'make', 'sense', 'actors', 'pretty', 'good', 'part', 'although', 'wes', 'bentley', 'seemed', 'playing', 'exact', 'character', 'american', 'beauty', 'new', 'neighborhood', 'biggest', 'kudos', 'go', 'sagemiller', 'holds', 'throughout', 'entire', 'film', 'actually', 'feeling', 'characters', 'unraveling', 'overall', 'film', 'doesnt', 'stick', 'doesnt', 'entertain', 'confusing', 'rarely', 'excites', 'feels', 'pretty', 'redundant', 'runtime', 'despite', 'pretty', 'cool', 'ending', 'explanation', 'craziness', 'came', 'oh', 'way', 'horror', 'teen', 'slasher', 'flick', 'packaged', 'look', 'way', 'someone', 'apparently', 'assuming', 'genre', 'still', 'hot', 'kids', 'also', 'wrapped', 'production', 'two', 'years', 'ago', 'sitting', 'shelves', 'ever', 'since', 'whatever', 'skip', 'wheres', 'joblo', 'coming', 'nightmare', 'elm', 'street', 'blair', 'witch', 'crow', 'crow', 'salvation', 'lost', 'highway', 'memento', 'others', 'stir', 'echoes']\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "import re\n",
    "\n",
    "# load doc into memory\n",
    "def load_doc(filename):\n",
    "\t# open the file as read only\n",
    "\tfile = open(filename, 'r')\n",
    "\t# read all text\n",
    "\ttext = file.read()\n",
    "\t# close the file\n",
    "\tfile.close()\n",
    "\treturn text\n",
    "\n",
    "# load the document\n",
    "filename = 'txt_sentoken/neg/cv000_29416.txt'\n",
    "text = load_doc(filename)\n",
    "# split into tokens by white space\n",
    "tokens = text.split()\n",
    "# prepare regex for char filtering\n",
    "re_punc = re.compile('[%s]' % re.escape(string.punctuation))\n",
    "# remove punctuation from each word\n",
    "tokens = [re_punc.sub('', w) for w in tokens]\n",
    "# remove remaining tokens that are not alphabetic\n",
    "tokens = [word for word in tokens if word.isalpha()]\n",
    "# filter out stop words\n",
    "stop_words = set(stopwords.words('english'))\n",
    "tokens = [w for w in tokens if not w in stop_words]\n",
    "# filter out short tokens\n",
    "tokens = [word for word in tokens if len(word) > 1]\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### cleaning with function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['films', 'adapted', 'comic', 'books', 'plenty', 'success', 'whether', 'theyre', 'superheroes', 'batman', 'superman', 'spawn', 'geared', 'toward', 'kids', 'casper', 'arthouse', 'crowd', 'ghost', 'world', 'theres', 'never', 'really', 'comic', 'book', 'like', 'hell', 'starters', 'created', 'alan', 'moore', 'eddie', 'campbell', 'brought', 'medium', 'whole', 'new', 'level', 'mid', 'series', 'called', 'watchmen', 'say', 'moore', 'campbell', 'thoroughly', 'researched', 'subject', 'jack', 'ripper', 'would', 'like', 'saying', 'michael', 'jackson', 'starting', 'look', 'little', 'odd', 'book', 'graphic', 'novel', 'pages', 'long', 'includes', 'nearly', 'consist', 'nothing', 'footnotes', 'words', 'dont', 'dismiss', 'film', 'source', 'get', 'past', 'whole', 'comic', 'book', 'thing', 'might', 'find', 'another', 'stumbling', 'block', 'hells', 'directors', 'albert', 'allen', 'hughes', 'getting', 'hughes', 'brothers', 'direct', 'seems', 'almost', 'ludicrous', 'casting', 'carrot', 'top', 'well', 'anything', 'riddle', 'better', 'direct', 'film', 'thats', 'set', 'ghetto', 'features', 'really', 'violent', 'street', 'crime', 'mad', 'geniuses', 'behind', 'menace', 'ii', 'society', 'ghetto', 'question', 'course', 'whitechapel', 'londons', 'east', 'end', 'filthy', 'sooty', 'place', 'whores', 'called', 'unfortunates', 'starting', 'get', 'little', 'nervous', 'mysterious', 'psychopath', 'carving', 'profession', 'surgical', 'precision', 'first', 'stiff', 'turns', 'copper', 'peter', 'godley', 'robbie', 'coltrane', 'world', 'enough', 'calls', 'inspector', 'frederick', 'abberline', 'johnny', 'depp', 'blow', 'crack', 'case', 'abberline', 'widower', 'prophetic', 'dreams', 'unsuccessfully', 'tries', 'quell', 'copious', 'amounts', 'absinthe', 'opium', 'upon', 'arriving', 'whitechapel', 'befriends', 'unfortunate', 'named', 'mary', 'kelly', 'heather', 'graham', 'say', 'isnt', 'proceeds', 'investigate', 'horribly', 'gruesome', 'crimes', 'even', 'police', 'surgeon', 'cant', 'stomach', 'dont', 'think', 'anyone', 'needs', 'briefed', 'jack', 'ripper', 'wont', 'go', 'particulars', 'say', 'moore', 'campbell', 'unique', 'interesting', 'theory', 'identity', 'killer', 'reasons', 'chooses', 'slay', 'comic', 'dont', 'bother', 'cloaking', 'identity', 'ripper', 'screenwriters', 'terry', 'hayes', 'vertical', 'limit', 'rafael', 'yglesias', 'les', 'mis', 'rables', 'good', 'job', 'keeping', 'hidden', 'viewers', 'end', 'funny', 'watch', 'locals', 'blindly', 'point', 'finger', 'blame', 'jews', 'indians', 'englishman', 'could', 'never', 'capable', 'committing', 'ghastly', 'acts', 'hells', 'ending', 'whistling', 'stonecutters', 'song', 'simpsons', 'days', 'holds', 'back', 'electric', 'carwho', 'made', 'steve', 'guttenberg', 'star', 'dont', 'worry', 'itll', 'make', 'sense', 'see', 'onto', 'hells', 'appearance', 'certainly', 'dark', 'bleak', 'enough', 'surprising', 'see', 'much', 'looks', 'like', 'tim', 'burton', 'film', 'planet', 'apes', 'times', 'seems', 'like', 'sleepy', 'hollow', 'print', 'saw', 'wasnt', 'completely', 'finished', 'color', 'music', 'finalized', 'comments', 'marilyn', 'manson', 'cinematographer', 'peter', 'deming', 'dont', 'say', 'word', 'ably', 'captures', 'dreariness', 'victorianera', 'london', 'helped', 'make', 'flashy', 'killing', 'scenes', 'remind', 'crazy', 'flashbacks', 'twin', 'peaks', 'even', 'though', 'violence', 'film', 'pales', 'comparison', 'blackandwhite', 'comic', 'oscar', 'winner', 'martin', 'childs', 'shakespeare', 'love', 'production', 'design', 'turns', 'original', 'prague', 'surroundings', 'one', 'creepy', 'place', 'even', 'acting', 'hell', 'solid', 'dreamy', 'depp', 'turning', 'typically', 'strong', 'performance', 'deftly', 'handling', 'british', 'accent', 'ians', 'holm', 'joe', 'goulds', 'secret', 'richardson', 'dalmatians', 'log', 'great', 'supporting', 'roles', 'big', 'surprise', 'graham', 'cringed', 'first', 'time', 'opened', 'mouth', 'imagining', 'attempt', 'irish', 'accent', 'actually', 'wasnt', 'half', 'bad', 'film', 'however', 'good', 'strong', 'violencegore', 'sexuality', 'language', 'drug', 'content']\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "import re\n",
    "\n",
    "# load doc into memory\n",
    "def load_doc(filename):\n",
    "\t# open the file as read only\n",
    "\tfile = open(filename, 'r')\n",
    "\t# read all text\n",
    "\ttext = file.read()\n",
    "\t# close the file\n",
    "\tfile.close()\n",
    "\treturn text\n",
    "\n",
    "# turn a doc into clean tokens\n",
    "def clean_doc(doc):\n",
    "\t# split into tokens by white space\n",
    "\ttokens = doc.split()\n",
    "\t# prepare regex for char filtering\n",
    "\tre_punc = re.compile('[%s]' % re.escape(string.punctuation))\n",
    "\t# remove punctuation from each word\n",
    "\ttokens = [re_punc.sub('', w) for w in tokens]\n",
    "\t# remove remaining tokens that are not alphabetic\n",
    "\ttokens = [word for word in tokens if word.isalpha()]\n",
    "\t# filter out stop words\n",
    "\tstop_words = set(stopwords.words('english'))\n",
    "\ttokens = [w for w in tokens if not w in stop_words]\n",
    "\t# filter out short tokens\n",
    "\ttokens = [word for word in tokens if len(word) > 1]\n",
    "\treturn tokens\n",
    "\n",
    "# load the document\n",
    "filename = 'txt_sentoken/pos/cv000_29590.txt'\n",
    "text = load_doc(filename)\n",
    "tokens = clean_doc(text)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### clean and build vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "46557\n",
      "[('film', 8860), ('one', 5521), ('movie', 5440), ('like', 3553), ('even', 2555), ('good', 2320), ('time', 2283), ('story', 2118), ('films', 2102), ('would', 2042), ('much', 2024), ('also', 1965), ('characters', 1947), ('get', 1921), ('character', 1906), ('two', 1825), ('first', 1768), ('see', 1730), ('well', 1694), ('way', 1668), ('make', 1590), ('really', 1563), ('little', 1491), ('life', 1472), ('plot', 1451), ('people', 1420), ('movies', 1416), ('could', 1395), ('bad', 1374), ('scene', 1373), ('never', 1364), ('best', 1301), ('new', 1277), ('many', 1268), ('doesnt', 1267), ('man', 1266), ('scenes', 1265), ('dont', 1210), ('know', 1207), ('hes', 1150), ('great', 1141), ('another', 1111), ('love', 1089), ('action', 1078), ('go', 1075), ('us', 1065), ('director', 1056), ('something', 1048), ('end', 1047), ('still', 1038)]\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "import re\n",
    "from os import listdir\n",
    "from collections import Counter\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# load doc into memory\n",
    "def load_doc(filename):\n",
    "\t# open the file as read only\n",
    "\tfile = open(filename, 'r')\n",
    "\t# read all text\n",
    "\ttext = file.read()\n",
    "\t# close the file\n",
    "\tfile.close()\n",
    "\treturn text\n",
    "\n",
    "# turn a doc into clean tokens\n",
    "def clean_doc(doc):\n",
    "\t# split into tokens by white space\n",
    "\ttokens = doc.split()\n",
    "\t# prepare regex for char filtering\n",
    "\tre_punc = re.compile('[%s]' % re.escape(string.punctuation))\n",
    "\t# remove punctuation from each word\n",
    "\ttokens = [re_punc.sub('', w) for w in tokens]\n",
    "\t# remove remaining tokens that are not alphabetic\n",
    "\ttokens = [word for word in tokens if word.isalpha()]\n",
    "\t# filter out stop words\n",
    "\tstop_words = set(stopwords.words('english'))\n",
    "\ttokens = [w for w in tokens if not w in stop_words]\n",
    "\t# filter out short tokens\n",
    "\ttokens = [word for word in tokens if len(word) > 1]\n",
    "\treturn tokens\n",
    "\n",
    "# load doc and add to vocab\n",
    "def add_doc_to_vocab(filename, vocab):\n",
    "\t# load doc\n",
    "\tdoc = load_doc(filename)\n",
    "\t# clean doc\n",
    "\ttokens = clean_doc(doc)\n",
    "\t# update counts\n",
    "\tvocab.update(tokens)\n",
    "\n",
    "# load all docs in a directory\n",
    "def process_docs(directory, vocab):\n",
    "\t# walk through all files in the folder\n",
    "\tfor filename in listdir(directory):\n",
    "\t\t# skip files that do not have the right extension\n",
    "\t\tif not filename.endswith(\".txt\"):\n",
    "\t\t\tnext\n",
    "\t\t# create the full path of the file to open\n",
    "\t\tpath = directory + '/' + filename\n",
    "\t\t# add doc to vocab\n",
    "\t\tadd_doc_to_vocab(path, vocab)\n",
    "\n",
    "# define vocab\n",
    "vocab = Counter()\n",
    "# add all docs to vocab\n",
    "process_docs('txt_sentoken/neg', vocab)\n",
    "process_docs('txt_sentoken/pos', vocab)\n",
    "# print the size of the vocab\n",
    "print(len(vocab))\n",
    "# print the top words in the vocab\n",
    "print(vocab.most_common(50))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### build vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "46557\n",
      "[('film', 8860), ('one', 5521), ('movie', 5440), ('like', 3553), ('even', 2555), ('good', 2320), ('time', 2283), ('story', 2118), ('films', 2102), ('would', 2042), ('much', 2024), ('also', 1965), ('characters', 1947), ('get', 1921), ('character', 1906), ('two', 1825), ('first', 1768), ('see', 1730), ('well', 1694), ('way', 1668), ('make', 1590), ('really', 1563), ('little', 1491), ('life', 1472), ('plot', 1451), ('people', 1420), ('movies', 1416), ('could', 1395), ('bad', 1374), ('scene', 1373), ('never', 1364), ('best', 1301), ('new', 1277), ('many', 1268), ('doesnt', 1267), ('man', 1266), ('scenes', 1265), ('dont', 1210), ('know', 1207), ('hes', 1150), ('great', 1141), ('another', 1111), ('love', 1089), ('action', 1078), ('go', 1075), ('us', 1065), ('director', 1056), ('something', 1048), ('end', 1047), ('still', 1038)]\n",
      "14803\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "import re\n",
    "from os import listdir\n",
    "from collections import Counter\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# load doc into memory\n",
    "def load_doc(filename):\n",
    "\t# open the file as read only\n",
    "\tfile = open(filename, 'r')\n",
    "\t# read all text\n",
    "\ttext = file.read()\n",
    "\t# close the file\n",
    "\tfile.close()\n",
    "\treturn text\n",
    "\n",
    "# turn a doc into clean tokens\n",
    "def clean_doc(doc):\n",
    "\t# split into tokens by white space\n",
    "\ttokens = doc.split()\n",
    "\t# prepare regex for char filtering\n",
    "\tre_punc = re.compile('[%s]' % re.escape(string.punctuation))\n",
    "\t# remove punctuation from each word\n",
    "\ttokens = [re_punc.sub('', w) for w in tokens]\n",
    "\t# remove remaining tokens that are not alphabetic\n",
    "\ttokens = [word for word in tokens if word.isalpha()]\n",
    "\t# filter out stop words\n",
    "\tstop_words = set(stopwords.words('english'))\n",
    "\ttokens = [w for w in tokens if not w in stop_words]\n",
    "\t# filter out short tokens\n",
    "\ttokens = [word for word in tokens if len(word) > 1]\n",
    "\treturn tokens\n",
    "\n",
    "# load doc and add to vocab\n",
    "def add_doc_to_vocab(filename, vocab):\n",
    "\t# load doc\n",
    "\tdoc = load_doc(filename)\n",
    "\t# clean doc\n",
    "\ttokens = clean_doc(doc)\n",
    "\t# update counts\n",
    "\tvocab.update(tokens)\n",
    "\n",
    "# load all docs in a directory\n",
    "def process_docs(directory, vocab):\n",
    "\t# walk through all files in the folder\n",
    "\tfor filename in listdir(directory):\n",
    "\t\t# skip files that do not have the right extension\n",
    "\t\tif not filename.endswith(\".txt\"):\n",
    "\t\t\tnext\n",
    "\t\t# create the full path of the file to open\n",
    "\t\tpath = directory + '/' + filename\n",
    "\t\t# add doc to vocab\n",
    "\t\tadd_doc_to_vocab(path, vocab)\n",
    "\n",
    "# save list to file\n",
    "def save_list(lines, filename):\n",
    "\tdata = '\\n'.join(lines)\n",
    "\tfile = open(filename, 'w')\n",
    "\tfile.write(data)\n",
    "\tfile.close()\n",
    "\n",
    "# define vocab\n",
    "vocab = Counter()\n",
    "# add all docs to vocab\n",
    "process_docs('txt_sentoken/neg', vocab)\n",
    "process_docs('txt_sentoken/pos', vocab)\n",
    "# print the size of the vocab\n",
    "print(len(vocab))\n",
    "# print the top words in the vocab\n",
    "print(vocab.most_common(50))\n",
    "# keep tokens with > 5 occurrence\n",
    "min_occurrence = 5\n",
    "tokens = [k for k,c in vocab.items() if c >= min_occurrence]\n",
    "print(len(tokens))\n",
    "# save tokens to a vocabulary file\n",
    "save_list(tokens, 'vocab.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### load vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# load doc into memory\n",
    "def load_doc(filename):\n",
    "\t# open the file as read only\n",
    "\tfile = open(filename, 'r')\n",
    "\t# read all text\n",
    "\ttext = file.read()\n",
    "\t# close the file\n",
    "\tfile.close()\n",
    "\treturn text\n",
    "\n",
    "# load vocabulary\n",
    "vocab_filename = 'vocab.txt'\n",
    "vocab = load_doc(vocab_filename)\n",
    "vocab = vocab.split()\n",
    "vocab = set(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import string\n",
    "import re\n",
    "from os import listdir\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# load doc into memory\n",
    "def load_doc(filename):\n",
    "\t# open the file as read only\n",
    "\tfile = open(filename, 'r')\n",
    "\t# read all text\n",
    "\ttext = file.read()\n",
    "\t# close the file\n",
    "\tfile.close()\n",
    "\treturn text\n",
    "\n",
    "# turn a doc into clean tokens\n",
    "def clean_doc(doc):\n",
    "\t# split into tokens by white space\n",
    "\ttokens = doc.split()\n",
    "\t# prepare regex for char filtering\n",
    "\tre_punc = re.compile('[%s]' % re.escape(string.punctuation))\n",
    "\t# remove punctuation from each word\n",
    "\ttokens = [re_punc.sub('', w) for w in tokens]\n",
    "\t# remove remaining tokens that are not alphabetic\n",
    "\ttokens = [word for word in tokens if word.isalpha()]\n",
    "\t# filter out stop words\n",
    "\tstop_words = set(stopwords.words('english'))\n",
    "\ttokens = [w for w in tokens if not w in stop_words]\n",
    "\t# filter out short tokens\n",
    "\ttokens = [word for word in tokens if len(word) > 1]\n",
    "\treturn tokens\n",
    "\n",
    "# save list to file\n",
    "def save_list(lines, filename):\n",
    "\tdata = '\\n'.join(lines)\n",
    "\tfile = open(filename, 'w')\n",
    "\tfile.write(data)\n",
    "\tfile.close()\n",
    "\n",
    "# load doc, clean and return line of tokens\n",
    "def doc_to_line(filename, vocab):\n",
    "\t# load the doc\n",
    "\tdoc = load_doc(filename)\n",
    "\t# clean doc\n",
    "\ttokens = clean_doc(doc)\n",
    "\t# filter by vocab\n",
    "\ttokens = [w for w in tokens if w in vocab]\n",
    "\treturn ' '.join(tokens)\n",
    "\n",
    "# load all docs in a directory\n",
    "def process_docs(directory, vocab):\n",
    "\tlines = list()\n",
    "\t# walk through all files in the folder\n",
    "\tfor filename in listdir(directory):\n",
    "\t\t# skip files that do not have the right extension\n",
    "\t\tif not filename.endswith(\".txt\"):\n",
    "\t\t\tnext\n",
    "\t\t# create the full path of the file to open\n",
    "\t\tpath = directory + '/' + filename\n",
    "\t\t# load and clean the doc\n",
    "\t\tline = doc_to_line(path, vocab)\n",
    "\t\t# add to list\n",
    "\t\tlines.append(line)\n",
    "\treturn lines\n",
    "\n",
    "# load vocabulary\n",
    "vocab_filename = 'vocab.txt'\n",
    "vocab = load_doc(vocab_filename)\n",
    "vocab = vocab.split()\n",
    "vocab = set(vocab)\n",
    "# prepare negative reviews\n",
    "negative_lines = process_docs('txt_sentoken/neg', vocab)\n",
    "save_list(negative_lines, 'negative.txt')\n",
    "# prepare positive reviews\n",
    "positive_lines = process_docs('txt_sentoken/pos', vocab)\n",
    "save_list(positive_lines, 'positive.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Develop a Neural Bag-of-Words Model for Sentiment Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### clean review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['films', 'adapted', 'comic', 'books', 'plenty', 'success', 'whether', 'theyre', 'superheroes', 'batman', 'superman', 'spawn', 'geared', 'toward', 'kids', 'casper', 'arthouse', 'crowd', 'ghost', 'world', 'theres', 'never', 'really', 'comic', 'book', 'like', 'hell', 'starters', 'created', 'alan', 'moore', 'eddie', 'campbell', 'brought', 'medium', 'whole', 'new', 'level', 'mid', 'series', 'called', 'watchmen', 'say', 'moore', 'campbell', 'thoroughly', 'researched', 'subject', 'jack', 'ripper', 'would', 'like', 'saying', 'michael', 'jackson', 'starting', 'look', 'little', 'odd', 'book', 'graphic', 'novel', 'pages', 'long', 'includes', 'nearly', 'consist', 'nothing', 'footnotes', 'words', 'dont', 'dismiss', 'film', 'source', 'get', 'past', 'whole', 'comic', 'book', 'thing', 'might', 'find', 'another', 'stumbling', 'block', 'hells', 'directors', 'albert', 'allen', 'hughes', 'getting', 'hughes', 'brothers', 'direct', 'seems', 'almost', 'ludicrous', 'casting', 'carrot', 'top', 'well', 'anything', 'riddle', 'better', 'direct', 'film', 'thats', 'set', 'ghetto', 'features', 'really', 'violent', 'street', 'crime', 'mad', 'geniuses', 'behind', 'menace', 'ii', 'society', 'ghetto', 'question', 'course', 'whitechapel', 'londons', 'east', 'end', 'filthy', 'sooty', 'place', 'whores', 'called', 'unfortunates', 'starting', 'get', 'little', 'nervous', 'mysterious', 'psychopath', 'carving', 'profession', 'surgical', 'precision', 'first', 'stiff', 'turns', 'copper', 'peter', 'godley', 'robbie', 'coltrane', 'world', 'enough', 'calls', 'inspector', 'frederick', 'abberline', 'johnny', 'depp', 'blow', 'crack', 'case', 'abberline', 'widower', 'prophetic', 'dreams', 'unsuccessfully', 'tries', 'quell', 'copious', 'amounts', 'absinthe', 'opium', 'upon', 'arriving', 'whitechapel', 'befriends', 'unfortunate', 'named', 'mary', 'kelly', 'heather', 'graham', 'say', 'isnt', 'proceeds', 'investigate', 'horribly', 'gruesome', 'crimes', 'even', 'police', 'surgeon', 'cant', 'stomach', 'dont', 'think', 'anyone', 'needs', 'briefed', 'jack', 'ripper', 'wont', 'go', 'particulars', 'say', 'moore', 'campbell', 'unique', 'interesting', 'theory', 'identity', 'killer', 'reasons', 'chooses', 'slay', 'comic', 'dont', 'bother', 'cloaking', 'identity', 'ripper', 'screenwriters', 'terry', 'hayes', 'vertical', 'limit', 'rafael', 'yglesias', 'les', 'mis', 'rables', 'good', 'job', 'keeping', 'hidden', 'viewers', 'end', 'funny', 'watch', 'locals', 'blindly', 'point', 'finger', 'blame', 'jews', 'indians', 'englishman', 'could', 'never', 'capable', 'committing', 'ghastly', 'acts', 'hells', 'ending', 'whistling', 'stonecutters', 'song', 'simpsons', 'days', 'holds', 'back', 'electric', 'carwho', 'made', 'steve', 'guttenberg', 'star', 'dont', 'worry', 'itll', 'make', 'sense', 'see', 'onto', 'hells', 'appearance', 'certainly', 'dark', 'bleak', 'enough', 'surprising', 'see', 'much', 'looks', 'like', 'tim', 'burton', 'film', 'planet', 'apes', 'times', 'seems', 'like', 'sleepy', 'hollow', 'print', 'saw', 'wasnt', 'completely', 'finished', 'color', 'music', 'finalized', 'comments', 'marilyn', 'manson', 'cinematographer', 'peter', 'deming', 'dont', 'say', 'word', 'ably', 'captures', 'dreariness', 'victorianera', 'london', 'helped', 'make', 'flashy', 'killing', 'scenes', 'remind', 'crazy', 'flashbacks', 'twin', 'peaks', 'even', 'though', 'violence', 'film', 'pales', 'comparison', 'blackandwhite', 'comic', 'oscar', 'winner', 'martin', 'childs', 'shakespeare', 'love', 'production', 'design', 'turns', 'original', 'prague', 'surroundings', 'one', 'creepy', 'place', 'even', 'acting', 'hell', 'solid', 'dreamy', 'depp', 'turning', 'typically', 'strong', 'performance', 'deftly', 'handling', 'british', 'accent', 'ians', 'holm', 'joe', 'goulds', 'secret', 'richardson', 'dalmatians', 'log', 'great', 'supporting', 'roles', 'big', 'surprise', 'graham', 'cringed', 'first', 'time', 'opened', 'mouth', 'imagining', 'attempt', 'irish', 'accent', 'actually', 'wasnt', 'half', 'bad', 'film', 'however', 'good', 'strong', 'violencegore', 'sexuality', 'language', 'drug', 'content']\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "import re\n",
    "\n",
    "# load doc into memory\n",
    "def load_doc(filename):\n",
    "\t# open the file as read only\n",
    "\tfile = open(filename, 'r')\n",
    "\t# read all text\n",
    "\ttext = file.read()\n",
    "\t# close the file\n",
    "\tfile.close()\n",
    "\treturn text\n",
    "\n",
    "# turn a doc into clean tokens\n",
    "def clean_doc(doc):\n",
    "\t# split into tokens by white space\n",
    "\ttokens = doc.split()\n",
    "\t# prepare regex for char filtering\n",
    "\tre_punc = re.compile('[%s]' % re.escape(string.punctuation))\n",
    "\t# remove punctuation from each word\n",
    "\ttokens = [re_punc.sub('', w) for w in tokens]\n",
    "\t# remove remaining tokens that are not alphabetic\n",
    "\ttokens = [word for word in tokens if word.isalpha()]\n",
    "\t# filter out stop words\n",
    "\tstop_words = set(stopwords.words('english'))\n",
    "\ttokens = [w for w in tokens if not w in stop_words]\n",
    "\t# filter out short tokens\n",
    "\ttokens = [word for word in tokens if len(word) > 1]\n",
    "\treturn tokens\n",
    "\n",
    "# load the document\n",
    "filename = 'txt_sentoken/pos/cv000_29590.txt'\n",
    "text = load_doc(filename)\n",
    "tokens = clean_doc(text)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### select vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "44276\n",
      "[('film', 7983), ('one', 4946), ('movie', 4826), ('like', 3201), ('even', 2262), ('good', 2080), ('time', 2041), ('story', 1907), ('films', 1873), ('would', 1844), ('much', 1824), ('also', 1757), ('characters', 1735), ('get', 1724), ('character', 1703), ('two', 1643), ('first', 1588), ('see', 1557), ('way', 1515), ('well', 1511), ('make', 1418), ('really', 1407), ('little', 1351), ('life', 1334), ('plot', 1288), ('people', 1269), ('could', 1248), ('bad', 1248), ('scene', 1241), ('movies', 1238), ('never', 1201), ('best', 1179), ('new', 1140), ('scenes', 1135), ('man', 1131), ('many', 1130), ('doesnt', 1118), ('know', 1092), ('dont', 1086), ('hes', 1024), ('great', 1014), ('another', 992), ('action', 985), ('love', 977), ('us', 967), ('go', 952), ('director', 948), ('end', 946), ('something', 945), ('still', 936)]\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "import re\n",
    "from os import listdir\n",
    "from collections import Counter\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# load doc into memory\n",
    "def load_doc(filename):\n",
    "\t# open the file as read only\n",
    "\tfile = open(filename, 'r')\n",
    "\t# read all text\n",
    "\ttext = file.read()\n",
    "\t# close the file\n",
    "\tfile.close()\n",
    "\treturn text\n",
    "\n",
    "# turn a doc into clean tokens\n",
    "def clean_doc(doc):\n",
    "\t# split into tokens by white space\n",
    "\ttokens = doc.split()\n",
    "\t# prepare regex for char filtering\n",
    "\tre_punc = re.compile('[%s]' % re.escape(string.punctuation))\n",
    "\t# remove punctuation from each word\n",
    "\ttokens = [re_punc.sub('', w) for w in tokens]\n",
    "\t# remove remaining tokens that are not alphabetic\n",
    "\ttokens = [word for word in tokens if word.isalpha()]\n",
    "\t# filter out stop words\n",
    "\tstop_words = set(stopwords.words('english'))\n",
    "\ttokens = [w for w in tokens if not w in stop_words]\n",
    "\t# filter out short tokens\n",
    "\ttokens = [word for word in tokens if len(word) > 1]\n",
    "\treturn tokens\n",
    "\n",
    "# load doc and add to vocab\n",
    "def add_doc_to_vocab(filename, vocab):\n",
    "\t# load doc\n",
    "\tdoc = load_doc(filename)\n",
    "\t# clean doc\n",
    "\ttokens = clean_doc(doc)\n",
    "\t# update counts\n",
    "\tvocab.update(tokens)\n",
    "\n",
    "# load all docs in a directory\n",
    "def process_docs(directory, vocab):\n",
    "\t# walk through all files in the folder\n",
    "\tfor filename in listdir(directory):\n",
    "\t\t# skip any reviews in the test set\n",
    "\t\tif filename.startswith('cv9'):\n",
    "\t\t\tcontinue\n",
    "\t\t# create the full path of the file to open\n",
    "\t\tpath = directory + '/' + filename\n",
    "\t\t# add doc to vocab\n",
    "\t\tadd_doc_to_vocab(path, vocab)\n",
    "\n",
    "# define vocab\n",
    "vocab = Counter()\n",
    "# add all docs to vocab\n",
    "process_docs('txt_sentoken/pos', vocab)\n",
    "process_docs('txt_sentoken/neg', vocab)\n",
    "# print the size of the vocab\n",
    "print(len(vocab))\n",
    "# print the top words in the vocab\n",
    "print(vocab.most_common(50))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### filter vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "44276\n",
      "25767\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "import re\n",
    "from os import listdir\n",
    "from collections import Counter\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# load doc into memory\n",
    "def load_doc(filename):\n",
    "\t# open the file as read only\n",
    "\tfile = open(filename, 'r')\n",
    "\t# read all text\n",
    "\ttext = file.read()\n",
    "\t# close the file\n",
    "\tfile.close()\n",
    "\treturn text\n",
    "\n",
    "# turn a doc into clean tokens\n",
    "def clean_doc(doc):\n",
    "\t# split into tokens by white space\n",
    "\ttokens = doc.split()\n",
    "\t# prepare regex for char filtering\n",
    "\tre_punc = re.compile('[%s]' % re.escape(string.punctuation))\n",
    "\t# remove punctuation from each word\n",
    "\ttokens = [re_punc.sub('', w) for w in tokens]\n",
    "\t# remove remaining tokens that are not alphabetic\n",
    "\ttokens = [word for word in tokens if word.isalpha()]\n",
    "\t# filter out stop words\n",
    "\tstop_words = set(stopwords.words('english'))\n",
    "\ttokens = [w for w in tokens if not w in stop_words]\n",
    "\t# filter out short tokens\n",
    "\ttokens = [word for word in tokens if len(word) > 1]\n",
    "\treturn tokens\n",
    "\n",
    "# load doc and add to vocab\n",
    "def add_doc_to_vocab(filename, vocab):\n",
    "\t# load doc\n",
    "\tdoc = load_doc(filename)\n",
    "\t# clean doc\n",
    "\ttokens = clean_doc(doc)\n",
    "\t# update counts\n",
    "\tvocab.update(tokens)\n",
    "\n",
    "# load all docs in a directory\n",
    "def process_docs(directory, vocab):\n",
    "\t# walk through all files in the folder\n",
    "\tfor filename in listdir(directory):\n",
    "\t\t# skip any reviews in the test set\n",
    "\t\tif filename.startswith('cv9'):\n",
    "\t\t\tcontinue\n",
    "\t\t# create the full path of the file to open\n",
    "\t\tpath = directory + '/' + filename\n",
    "\t\t# add doc to vocab\n",
    "\t\tadd_doc_to_vocab(path, vocab)\n",
    "\n",
    "# save list to file\n",
    "def save_list(lines, filename):\n",
    "\t# convert lines to a single blob of text\n",
    "\tdata = '\\n'.join(lines)\n",
    "\t# open file\n",
    "\tfile = open(filename, 'w')\n",
    "\t# write text\n",
    "\tfile.write(data)\n",
    "\t# close file\n",
    "\tfile.close()\n",
    "\n",
    "# define vocab\n",
    "vocab = Counter()\n",
    "# add all docs to vocab\n",
    "process_docs('txt_sentoken/pos', vocab)\n",
    "process_docs('txt_sentoken/neg', vocab)\n",
    "# print the size of the vocab\n",
    "print(len(vocab))\n",
    "# keep tokens with a min occurrence\n",
    "min_occurrence = 2\n",
    "tokens = [k for k,c in vocab.items() if c >= min_occurrence]\n",
    "print(len(tokens))\n",
    "# save tokens to a vocabulary file\n",
    "save_list(tokens, 'vocab.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### filter all reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1800 1800\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "import re\n",
    "from os import listdir\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# load doc into memory\n",
    "def load_doc(filename):\n",
    "\t# open the file as read only\n",
    "\tfile = open(filename, 'r')\n",
    "\t# read all text\n",
    "\ttext = file.read()\n",
    "\t# close the file\n",
    "\tfile.close()\n",
    "\treturn text\n",
    "\n",
    "# turn a doc into clean tokens\n",
    "def clean_doc(doc):\n",
    "\t# split into tokens by white space\n",
    "\ttokens = doc.split()\n",
    "\t# prepare regex for char filtering\n",
    "\tre_punc = re.compile('[%s]' % re.escape(string.punctuation))\n",
    "\t# remove punctuation from each word\n",
    "\ttokens = [re_punc.sub('', w) for w in tokens]\n",
    "\t# remove remaining tokens that are not alphabetic\n",
    "\ttokens = [word for word in tokens if word.isalpha()]\n",
    "\t# filter out stop words\n",
    "\tstop_words = set(stopwords.words('english'))\n",
    "\ttokens = [w for w in tokens if not w in stop_words]\n",
    "\t# filter out short tokens\n",
    "\ttokens = [word for word in tokens if len(word) > 1]\n",
    "\treturn tokens\n",
    "\n",
    "# load doc, clean and return line of tokens\n",
    "def doc_to_line(filename, vocab):\n",
    "\t# load the doc\n",
    "\tdoc = load_doc(filename)\n",
    "\t# clean doc\n",
    "\ttokens = clean_doc(doc)\n",
    "\t# filter by vocab\n",
    "\ttokens = [w for w in tokens if w in vocab]\n",
    "\treturn ' '.join(tokens)\n",
    "\n",
    "# load all docs in a directory\n",
    "def process_docs(directory, vocab):\n",
    "\tlines = list()\n",
    "\t# walk through all files in the folder\n",
    "\tfor filename in listdir(directory):\n",
    "\t\t# skip any reviews in the test set\n",
    "\t\tif filename.startswith('cv9'):\n",
    "\t\t\tcontinue\n",
    "\t\t# create the full path of the file to open\n",
    "\t\tpath = directory + '/' + filename\n",
    "\t\t# load and clean the doc\n",
    "\t\tline = doc_to_line(path, vocab)\n",
    "\t\t# add to list\n",
    "\t\tlines.append(line)\n",
    "\treturn lines\n",
    "\n",
    "# load and clean a dataset\n",
    "def load_clean_dataset(vocab):\n",
    "\t# load documents\n",
    "\tneg = process_docs('txt_sentoken/neg', vocab)\n",
    "\tpos = process_docs('txt_sentoken/pos', vocab)\n",
    "\tdocs = neg + pos\n",
    "\t# prepare labels\n",
    "\tlabels = [0 for _ in range(len(neg))] + [1 for _ in range(len(pos))]\n",
    "\treturn docs, labels\n",
    "\n",
    "# load the vocabulary\n",
    "vocab_filename = 'vocab.txt'\n",
    "vocab = load_doc(vocab_filename)\n",
    "vocab = vocab.split()\n",
    "vocab = set(vocab)\n",
    "# load all training reviews\n",
    "docs, labels = load_clean_dataset(vocab)\n",
    "# summarize what we have\n",
    "print(len(docs), len(labels))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### prepre data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1800, 25768) (200, 25768)\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "import re\n",
    "from os import listdir\n",
    "from numpy import array\n",
    "from nltk.corpus import stopwords\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "\n",
    "# load doc into memory\n",
    "def load_doc(filename):\n",
    "\t# open the file as read only\n",
    "\tfile = open(filename, 'r')\n",
    "\t# read all text\n",
    "\ttext = file.read()\n",
    "\t# close the file\n",
    "\tfile.close()\n",
    "\treturn text\n",
    "\n",
    "# turn a doc into clean tokens\n",
    "def clean_doc(doc):\n",
    "\t# split into tokens by white space\n",
    "\ttokens = doc.split()\n",
    "\t# prepare regex for char filtering\n",
    "\tre_punc = re.compile('[%s]' % re.escape(string.punctuation))\n",
    "\t# remove punctuation from each word\n",
    "\ttokens = [re_punc.sub('', w) for w in tokens]\n",
    "\t# remove remaining tokens that are not alphabetic\n",
    "\ttokens = [word for word in tokens if word.isalpha()]\n",
    "\t# filter out stop words\n",
    "\tstop_words = set(stopwords.words('english'))\n",
    "\ttokens = [w for w in tokens if not w in stop_words]\n",
    "\t# filter out short tokens\n",
    "\ttokens = [word for word in tokens if len(word) > 1]\n",
    "\treturn tokens\n",
    "\n",
    "# load doc, clean and return line of tokens\n",
    "def doc_to_line(filename, vocab):\n",
    "\t# load the doc\n",
    "\tdoc = load_doc(filename)\n",
    "\t# clean doc\n",
    "\ttokens = clean_doc(doc)\n",
    "\t# filter by vocab\n",
    "\ttokens = [w for w in tokens if w in vocab]\n",
    "\treturn ' '.join(tokens)\n",
    "\n",
    "# load all docs in a directory\n",
    "def process_docs(directory, vocab, is_train):\n",
    "\tlines = list()\n",
    "\t# walk through all files in the folder\n",
    "\tfor filename in listdir(directory):\n",
    "\t\t# skip any reviews in the test set\n",
    "\t\tif is_train and filename.startswith('cv9'):\n",
    "\t\t\tcontinue\n",
    "\t\tif not is_train and not filename.startswith('cv9'):\n",
    "\t\t\tcontinue\n",
    "\t\t# create the full path of the file to open\n",
    "\t\tpath = directory + '/' + filename\n",
    "\t\t# load and clean the doc\n",
    "\t\tline = doc_to_line(path, vocab)\n",
    "\t\t# add to list\n",
    "\t\tlines.append(line)\n",
    "\treturn lines\n",
    "\n",
    "# load and clean a dataset\n",
    "def load_clean_dataset(vocab, is_train):\n",
    "\t# load documents\n",
    "\tneg = process_docs('txt_sentoken/neg', vocab, is_train)\n",
    "\tpos = process_docs('txt_sentoken/pos', vocab, is_train)\n",
    "\tdocs = neg + pos\n",
    "\t# prepare labels\n",
    "\tlabels = array([0 for _ in range(len(neg))] + [1 for _ in range(len(pos))])\n",
    "\treturn docs, labels\n",
    "\n",
    "# fit a tokenizer\n",
    "def create_tokenizer(lines):\n",
    "\ttokenizer = Tokenizer()\n",
    "\ttokenizer.fit_on_texts(lines)\n",
    "\treturn tokenizer\n",
    "\n",
    "# load the vocabulary\n",
    "vocab_filename = 'vocab.txt'\n",
    "vocab = load_doc(vocab_filename)\n",
    "vocab = set(vocab.split())\n",
    "# load all reviews\n",
    "train_docs, ytrain = load_clean_dataset(vocab, True)\n",
    "test_docs, ytest = load_clean_dataset(vocab, False)\n",
    "# create the tokenizer\n",
    "tokenizer = create_tokenizer(train_docs)\n",
    "# encode data\n",
    "Xtrain = tokenizer.texts_to_matrix(train_docs, mode='freq')\n",
    "Xtest = tokenizer.texts_to_matrix(test_docs, mode='freq')\n",
    "print(Xtrain.shape, Xtest.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### mlp_bow_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 50)                1288450   \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 51        \n",
      "=================================================================\n",
      "Total params: 1,288,501\n",
      "Trainable params: 1,288,501\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "Failed to import pydot. You must install pydot and graphviz for `pydotprint` to work.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32mC:\\Users\\yogesh\\Anaconda3\\lib\\site-packages\\pydot.py\u001b[0m in \u001b[0;36mcreate\u001b[0;34m(self, prog, format)\u001b[0m\n\u001b[1;32m   1877\u001b[0m                 \u001b[0mshell\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1878\u001b[0;31m                 stderr=subprocess.PIPE, stdout=subprocess.PIPE)\n\u001b[0m\u001b[1;32m   1879\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mOSError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\yogesh\\Anaconda3\\lib\\subprocess.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, args, bufsize, executable, stdin, stdout, stderr, preexec_fn, close_fds, shell, cwd, env, universal_newlines, startupinfo, creationflags, restore_signals, start_new_session, pass_fds, encoding, errors)\u001b[0m\n\u001b[1;32m    708\u001b[0m                                 \u001b[0merrread\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0merrwrite\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 709\u001b[0;31m                                 restore_signals, start_new_session)\n\u001b[0m\u001b[1;32m    710\u001b[0m         \u001b[1;32mexcept\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\yogesh\\Anaconda3\\lib\\subprocess.py\u001b[0m in \u001b[0;36m_execute_child\u001b[0;34m(self, args, executable, preexec_fn, close_fds, pass_fds, cwd, env, startupinfo, creationflags, shell, p2cread, p2cwrite, c2pread, c2pwrite, errread, errwrite, unused_restore_signals, unused_start_new_session)\u001b[0m\n\u001b[1;32m    996\u001b[0m                                          \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfspath\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcwd\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mcwd\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 997\u001b[0;31m                                          startupinfo)\n\u001b[0m\u001b[1;32m    998\u001b[0m             \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [WinError 2] The system cannot find the file specified",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mException\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32mC:\\Users\\yogesh\\Anaconda3\\lib\\site-packages\\keras\\utils\\vis_utils.py\u001b[0m in \u001b[0;36m_check_pydot\u001b[0;34m()\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[1;31m# to check the pydot/graphviz installation.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m         \u001b[0mpydot\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDot\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcreate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpydot\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\yogesh\\Anaconda3\\lib\\site-packages\\pydot.py\u001b[0m in \u001b[0;36mcreate\u001b[0;34m(self, prog, format)\u001b[0m\n\u001b[1;32m   1882\u001b[0m                     '\"{prog}\" not found in path.'.format(\n\u001b[0;32m-> 1883\u001b[0;31m                         prog=prog))\n\u001b[0m\u001b[1;32m   1884\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mException\u001b[0m: \"dot.exe\" not found in path.",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-63bc13fe4640>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    107\u001b[0m \u001b[1;31m# define the model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m \u001b[0mn_words\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mXtest\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 109\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdefine_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_words\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    110\u001b[0m \u001b[1;31m# fit network\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mXtrain\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mytrain\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-1-63bc13fe4640>\u001b[0m in \u001b[0;36mdefine_model\u001b[0;34m(n_words)\u001b[0m\n\u001b[1;32m     90\u001b[0m         \u001b[1;31m# summarize defined model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msummary\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m---> 92\u001b[0;31m         \u001b[0mplot_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mto_file\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'model.png'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshow_shapes\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     93\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\yogesh\\Anaconda3\\lib\\site-packages\\keras\\utils\\vis_utils.py\u001b[0m in \u001b[0;36mplot_model\u001b[0;34m(model, to_file, show_shapes, show_layer_names, rankdir)\u001b[0m\n\u001b[1;32m    129\u001b[0m             \u001b[1;34m'LR'\u001b[0m \u001b[0mcreates\u001b[0m \u001b[0ma\u001b[0m \u001b[0mhorizontal\u001b[0m \u001b[0mplot\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m     \"\"\"\n\u001b[0;32m--> 131\u001b[0;31m     \u001b[0mdot\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel_to_dot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshow_shapes\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshow_layer_names\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrankdir\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    132\u001b[0m     \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mextension\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplitext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mto_file\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mextension\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\yogesh\\Anaconda3\\lib\\site-packages\\keras\\utils\\vis_utils.py\u001b[0m in \u001b[0;36mmodel_to_dot\u001b[0;34m(model, show_shapes, show_layer_names, rankdir)\u001b[0m\n\u001b[1;32m     50\u001b[0m     \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mSequential\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m     \u001b[0m_check_pydot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m     \u001b[0mdot\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpydot\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m     \u001b[0mdot\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'rankdir'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrankdir\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\yogesh\\Anaconda3\\lib\\site-packages\\keras\\utils\\vis_utils.py\u001b[0m in \u001b[0;36m_check_pydot\u001b[0;34m()\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[1;31m# pydot raises a generic Exception here,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[1;31m# so no specific class can be caught.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m         raise ImportError('Failed to import pydot. You must install pydot'\n\u001b[0m\u001b[1;32m     28\u001b[0m                           ' and graphviz for `pydotprint` to work.')\n\u001b[1;32m     29\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: Failed to import pydot. You must install pydot and graphviz for `pydotprint` to work."
     ]
    }
   ],
   "source": [
    "import string\n",
    "import re\n",
    "from os import listdir\n",
    "from numpy import array\n",
    "from nltk.corpus import stopwords\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.utils.vis_utils import plot_model\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "\n",
    "# load doc into memory\n",
    "def load_doc(filename):\n",
    "\t# open the file as read only\n",
    "\tfile = open(filename, 'r')\n",
    "\t# read all text\n",
    "\ttext = file.read()\n",
    "\t# close the file\n",
    "\tfile.close()\n",
    "\treturn text\n",
    "\n",
    "# turn a doc into clean tokens\n",
    "def clean_doc(doc):\n",
    "\t# split into tokens by white space\n",
    "\ttokens = doc.split()\n",
    "\t# prepare regex for char filtering\n",
    "\tre_punc = re.compile('[%s]' % re.escape(string.punctuation))\n",
    "\t# remove punctuation from each word\n",
    "\ttokens = [re_punc.sub('', w) for w in tokens]\n",
    "\t# remove remaining tokens that are not alphabetic\n",
    "\ttokens = [word for word in tokens if word.isalpha()]\n",
    "\t# filter out stop words\n",
    "\tstop_words = set(stopwords.words('english'))\n",
    "\ttokens = [w for w in tokens if not w in stop_words]\n",
    "\t# filter out short tokens\n",
    "\ttokens = [word for word in tokens if len(word) > 1]\n",
    "\treturn tokens\n",
    "\n",
    "# load doc, clean and return line of tokens\n",
    "def doc_to_line(filename, vocab):\n",
    "\t# load the doc\n",
    "\tdoc = load_doc(filename)\n",
    "\t# clean doc\n",
    "\ttokens = clean_doc(doc)\n",
    "\t# filter by vocab\n",
    "\ttokens = [w for w in tokens if w in vocab]\n",
    "\treturn ' '.join(tokens)\n",
    "\n",
    "# load all docs in a directory\n",
    "def process_docs(directory, vocab, is_train):\n",
    "\tlines = list()\n",
    "\t# walk through all files in the folder\n",
    "\tfor filename in listdir(directory):\n",
    "\t\t# skip any reviews in the test set\n",
    "\t\tif is_train and filename.startswith('cv9'):\n",
    "\t\t\tcontinue\n",
    "\t\tif not is_train and not filename.startswith('cv9'):\n",
    "\t\t\tcontinue\n",
    "\t\t# create the full path of the file to open\n",
    "\t\tpath = directory + '/' + filename\n",
    "\t\t# load and clean the doc\n",
    "\t\tline = doc_to_line(path, vocab)\n",
    "\t\t# add to list\n",
    "\t\tlines.append(line)\n",
    "\treturn lines\n",
    "\n",
    "# load and clean a dataset\n",
    "def load_clean_dataset(vocab, is_train):\n",
    "\t# load documents\n",
    "\tneg = process_docs('txt_sentoken/neg', vocab, is_train)\n",
    "\tpos = process_docs('txt_sentoken/pos', vocab, is_train)\n",
    "\tdocs = neg + pos\n",
    "\t# prepare labels\n",
    "\tlabels = array([0 for _ in range(len(neg))] + [1 for _ in range(len(pos))])\n",
    "\treturn docs, labels\n",
    "\n",
    "# fit a tokenizer\n",
    "def create_tokenizer(lines):\n",
    "\ttokenizer = Tokenizer()\n",
    "\ttokenizer.fit_on_texts(lines)\n",
    "\treturn tokenizer\n",
    "\n",
    "# define the model\n",
    "def define_model(n_words):\n",
    "\t# define network\n",
    "\tmodel = Sequential()\n",
    "\tmodel.add(Dense(50, input_shape=(n_words,), activation='relu'))\n",
    "\tmodel.add(Dense(1, activation='sigmoid'))\n",
    "\t# compile network\n",
    "\tmodel.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\t# summarize defined model\n",
    "\tmodel.summary()\n",
    "\tplot_model(model, to_file='model.png', show_shapes=True)\n",
    "\treturn model\n",
    "\n",
    "# load the vocabulary\n",
    "vocab_filename = 'vocab.txt'\n",
    "vocab = load_doc(vocab_filename)\n",
    "vocab = set(vocab.split())\n",
    "# load all reviews\n",
    "train_docs, ytrain = load_clean_dataset(vocab, True)\n",
    "test_docs, ytest = load_clean_dataset(vocab, False)\n",
    "# create the tokenizer\n",
    "tokenizer = create_tokenizer(train_docs)\n",
    "# encode data\n",
    "Xtrain = tokenizer.texts_to_matrix(train_docs, mode='freq')\n",
    "Xtest = tokenizer.texts_to_matrix(test_docs, mode='freq')\n",
    "# define the model\n",
    "n_words = Xtest.shape[1]\n",
    "model = define_model(n_words)\n",
    "# fit network\n",
    "model.fit(Xtrain, ytrain, epochs=10, verbose=2)\n",
    "# evaluate\n",
    "loss, acc = model.evaluate(Xtest, ytest, verbose=0)\n",
    "print('Test Accuracy: %f' % (acc*100))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### compare encodings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pydot_ng'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-33e6c82052c3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[1;32mimport\u001b[0m \u001b[0mpydot_ng\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mpydot\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'pydot_ng'"
     ]
    }
   ],
   "source": [
    "import pydot_ng as pydot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
